{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13e128-d322-4b83-b2f5-e449217e0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def get_all_files_from_path(mypath):\n",
    "    filenames = [join(mypath, f) for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    return filenames\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "def get_article(articles):\n",
    "    result = {}\n",
    "    current_statue = \"(non-statute)\"\n",
    "    for i in re.split(r\"(.*)\", articles.strip()):\n",
    "        if len(i) == 0 or i == \"\\n\":\n",
    "            continue\n",
    "        if re.search(r\"^\\(.*\\)$\", i):\n",
    "            current_statue = i.strip()\n",
    "            if current_statue not in result:\n",
    "                result.update({current_statue: []})\n",
    "        else:\n",
    "            if current_statue not in result:\n",
    "                result.update({current_statue: []})\n",
    "            result[current_statue].append(i)\n",
    "    return result\n",
    "\n",
    "def build_test(filename):\n",
    "    result = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    data = BeautifulSoup(data, \"xml\").find_all('pair')\n",
    "    for i in data:\n",
    "        id = i.get('id')\n",
    "        result.update({id: {}})\n",
    "        result[id].update({\"label\": i.get('label')})\n",
    "        articles = i.find('t1').text.strip()\n",
    "        # articles = get_article(articles)\n",
    "        result[id].update({\"result\": articles})\n",
    "        result[id].update({\"content\": i.find('t2').text.strip()})\n",
    "    return result\n",
    "\n",
    "def write_json(filename, data):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "import xml.etree.ElementTree as Et\n",
    "import glob\n",
    "\n",
    "def format_first_line(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    results = []\n",
    "    for line in lines:\n",
    "        if line[0] == \"\":\n",
    "            continue\n",
    "        if line[0] == \"(\" and line[-1] == \")\":\n",
    "            continue\n",
    "        results.append(line)\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "def load_samples(filexml):\n",
    "    # try:\n",
    "    tree = Et.parse(filexml)\n",
    "    root = tree.getroot()\n",
    "    samples = []\n",
    "    for i in range(0, len(root)):\n",
    "        sample = {'result': []}\n",
    "        for j, e in enumerate(root[i]):\n",
    "            if e.tag == \"t1\":\n",
    "                sample['result'] = format_first_line(e.text.strip())\n",
    "            elif e.tag == \"t2\":\n",
    "                question = e.text.strip()\n",
    "                sample['content'] = question if len(question) > 0 else None\n",
    "        sample.update(\n",
    "            {'index': root[i].attrib['id'], 'label': root[i].attrib.get('label', \"N\")})\n",
    "        # filter the noise samples\n",
    "        if sample['content'] is not None:\n",
    "            samples.append(sample)\n",
    "        else:\n",
    "            print(\"[Important warning] samples {} is ignored\".format(sample))\n",
    "    return samples\n",
    "\n",
    "def load_test_data_samples(path_folder_base, test_id):\n",
    "    data = []\n",
    "    test = load_samples(f\"{path_folder_base}/riteval_{test_id}.xml\")\n",
    "    for file_path in glob.glob(f\"{path_folder_base}/riteval_{test_id}.xml\"):\n",
    "        data = data + load_samples(file_path)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_all_data_samples(path_folder_base):\n",
    "    data = []\n",
    "    for file_path in glob.glob(\"{}/*.xml\".format(path_folder_base)):\n",
    "        data = data + load_samples(file_path)\n",
    "    return data\n",
    "\n",
    "def check_false_labels(pred, false_labels):\n",
    "\tfor label in false_labels:\n",
    "\t\tif label in pred:\n",
    "\t\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def format_output(text):\n",
    "\tCLEANR = re.compile('<.*?>') \n",
    "\tcleantext = re.sub(CLEANR, '', text)\n",
    "\treturn cleantext.strip().lower()\n",
    "\n",
    "def readfile(filename):\n",
    "    f = open(filename)\n",
    "    data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_jsonl(file):\n",
    "    with open(file) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "def format_output(text):\n",
    "\tCLEANR = re.compile('<.*?>') \n",
    "\tcleantext = re.sub(CLEANR, '', text)\n",
    "\treturn cleantext.strip().lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9575f85-e409-4f35-9c32-8632c23b3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, BloomForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# import torch\n",
    "\n",
    "# model_name = \"google/flan-t5-xxl\"\n",
    "# cache_dir = \"/home/congnguyen/drive/.cache\"\n",
    "# # cache_dir = \".cache\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "# \t\tmodel_name, device_map=\"auto\", cache_dir=cache_dir, torch_dtype=torch.float16, load_in_8bit=True\n",
    "# \t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba6e88-ef4b-4f93-94fa-b375aac41b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen-72B-Chat\"\n",
    "cache_dir = \"/home/congnguyen/drive/.cache\"\n",
    "# cache_dir = \".cache\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Note: The default behavior now has injection attack prevention off.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-72B-Chat\", cache_dir=cache_dir, trust_remote_code=True)\n",
    "# device_map=\"auto\", \n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-72B-Chat\", cache_dir=cache_dir, device_map=\"auto\", \n",
    "                                             torch_dtype=torch.float16, trust_remote_code=True, load_in_4bit=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e6a8b-c1dd-4135-aaa7-9ce541e6239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../data/COLIEE2024statute_data-English/fewshot.json/riteval_R03_en.jsonl\"\n",
    "data = load_jsonl(test_path)\n",
    "data[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107f225-c585-43e3-8db0-11f64084409e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prompt = \"EXAM\\n1. Answer by yes or no.\\n\\nDocument: {{premise}}\\nQuestion: {{hypothesis}}? \"\n",
    "data = load_jsonl(test_path)\n",
    "count = 0\n",
    "for item in tqdm(data):\n",
    "    premise = item[\"result\"]\n",
    "    label = item[\"label\"]\n",
    "    hypothesis = item[\"content\"]\n",
    "    text = prompt.replace(\"{{premise}}\", premise).replace(\"{{hypothesis}}\", hypothesis)\n",
    "    response, history = model.chat(tokenizer, text, history=None)\n",
    "    outputs = format_output(response)\n",
    "    if count<1:\n",
    "        print(text)\n",
    "        print(outputs)\n",
    "    if \"yes\" in outputs or \"true\" in outputs:\n",
    "        out = \"Y\"\n",
    "    else:\n",
    "        out = \"N\"\n",
    "    if out == label:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce05fe-f282-4627-993b-724ea9ac5d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"EXAM\\n1. Answer by yes or no.\\n\\nDocument: {{premise}}\\nQuestion: {{hypothesis}}? \"\n",
    "torch.cuda.empty_cache()\n",
    "def predict(test_path, out_path):\n",
    "    files = get_all_files_from_path(test_path)\n",
    "    acc = {}\n",
    "    print(files)\n",
    "    for file in files:\n",
    "        data = load_jsonl(file)\n",
    "        count = 0\n",
    "        out_file = file.split(\"/\")[-1].replace(\".jsonl\", \".txt\")\n",
    "        f = open(out_path+out_file, \"w\")\n",
    "        for item in tqdm(data):\n",
    "            premise = item[\"result\"]\n",
    "            label = item[\"label\"]\n",
    "            hypothesis = item[\"content\"]\n",
    "            text = prompt.replace(\"{{premise}}\", premise).replace(\"{{hypothesis}}\", hypothesis)\n",
    "            response, history = model.chat(tokenizer, text, history=None)\n",
    "            torch.cuda.empty_cache()\n",
    "            outputs = format_output(response)\n",
    "            if count<100:\n",
    "                print(text)\n",
    "                print(outputs)\n",
    "            if \"yes\" in outputs or \"true\" in outputs:\n",
    "                out = \"Y\"\n",
    "            else:\n",
    "                out = \"N\"\n",
    "            if out == label:\n",
    "                count += 1\n",
    "            f.write(item[\"index\"]+\": \"+outputs+\"\\n\")\n",
    "        acc.update({out_file: count/len(data)})\n",
    "    write_json(out_path+\"acc.json\", acc)\n",
    "\n",
    "test_path = \"../data/COLIEE2024statute_data-English/test.json/\"\n",
    "out_path = \"../output/qwen/zeroshot/\"\n",
    "predict(test_path, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fcd400-05f6-4b47-9e46-166f39f6743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"A mandate terminates when:\\n(i) the mandator or mandatary dies;\\n(ii) the mandator or mandatary is subject to an order commencing bankruptcy proceeding;\\n(iii) the mandatary is subject to a decision for the commencement of guardianship.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8dd8ea-51a8-4463-a6e3-ff36889692ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sent + \"\\nAnalyze the structure following main premise, exception of each rule.\" \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ccef2-7bff-436c-a03d-2dbf7699ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "# outputs = model.generate(inputs, max_new_tokens=2048)\n",
    "# output_text = format_output(tokenizer.decode(outputs[0]).replace(text, \"\").split(\"\\n\")[-1])\n",
    "# output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058af73-e016-4e0e-8c1f-f169dbbf0db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

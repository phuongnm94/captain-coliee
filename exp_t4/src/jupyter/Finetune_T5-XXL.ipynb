{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704bd940-1d54-4365-9cf6-0e8620a336c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def get_all_files_from_path(mypath):\n",
    "    filenames = [join(mypath, f) for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    return filenames\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "def get_article(articles):\n",
    "    result = {}\n",
    "    current_statue = \"(non-statute)\"\n",
    "    for i in re.split(r\"(.*)\", articles.strip()):\n",
    "        if len(i) == 0 or i == \"\\n\":\n",
    "            continue\n",
    "        if re.search(r\"^\\(.*\\)$\", i):\n",
    "            current_statue = i.strip()\n",
    "            if current_statue not in result:\n",
    "                result.update({current_statue: []})\n",
    "        else:\n",
    "            if current_statue not in result:\n",
    "                result.update({current_statue: []})\n",
    "            result[current_statue].append(i)\n",
    "    return result\n",
    "\n",
    "def build_test(filename):\n",
    "    result = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    data = BeautifulSoup(data, \"xml\").find_all('pair')\n",
    "    for i in data:\n",
    "        id = i.get('id')\n",
    "        result.update({id: {}})\n",
    "        result[id].update({\"label\": i.get('label')})\n",
    "        articles = i.find('t1').text.strip()\n",
    "        # articles = get_article(articles)\n",
    "        result[id].update({\"result\": articles})\n",
    "        result[id].update({\"content\": i.find('t2').text.strip()})\n",
    "    return result\n",
    "\n",
    "def write_json(filename, data):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "import xml.etree.ElementTree as Et\n",
    "import glob\n",
    "\n",
    "def format_first_line(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    results = []\n",
    "    for line in lines:\n",
    "        if line[0] == \"\":\n",
    "            continue\n",
    "        if line[0] == \"(\" and line[-1] == \")\":\n",
    "            continue\n",
    "        results.append(line)\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "def load_samples(filexml):\n",
    "    # try:\n",
    "    tree = Et.parse(filexml)\n",
    "    root = tree.getroot()\n",
    "    samples = []\n",
    "    for i in range(0, len(root)):\n",
    "        sample = {'result': []}\n",
    "        for j, e in enumerate(root[i]):\n",
    "            if e.tag == \"t1\":\n",
    "                sample['result'] = format_first_line(e.text.strip())\n",
    "            elif e.tag == \"t2\":\n",
    "                question = e.text.strip()\n",
    "                sample['content'] = question if len(question) > 0 else None\n",
    "        sample.update(\n",
    "            {'index': root[i].attrib['id'], 'label': root[i].attrib.get('label', \"N\")})\n",
    "        # filter the noise samples\n",
    "        if sample['content'] is not None:\n",
    "            samples.append(sample)\n",
    "        else:\n",
    "            print(\"[Important warning] samples {} is ignored\".format(sample))\n",
    "    return samples\n",
    "\n",
    "def load_test_data_samples(path_folder_base, test_id):\n",
    "    data = []\n",
    "    test = load_samples(f\"{path_folder_base}/riteval_{test_id}.xml\")\n",
    "    for file_path in glob.glob(f\"{path_folder_base}/riteval_{test_id}.xml\"):\n",
    "        data = data + load_samples(file_path)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_all_data_samples(path_folder_base):\n",
    "    data = []\n",
    "    for file_path in glob.glob(\"{}/*.xml\".format(path_folder_base)):\n",
    "        data = data + load_samples(file_path)\n",
    "    return data\n",
    "\n",
    "def check_false_labels(pred, false_labels):\n",
    "\tfor label in false_labels:\n",
    "\t\tif label in pred:\n",
    "\t\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def format_output(text):\n",
    "\tCLEANR = re.compile('<.*?>') \n",
    "\tcleantext = re.sub(CLEANR, '', text)\n",
    "\treturn cleantext.strip().lower()\n",
    "\n",
    "def readfile(filename):\n",
    "    f = open(filename)\n",
    "    data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba40b32d-b82d-4491-8928-b1338bc3f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroShot = \"According to the given legal reasoning approach.\\nDocument: {{premise}}\\nQuestion: {{hypothesis}}? True or False\"\n",
    "import jsonlines\n",
    "\n",
    "train_path = get_all_files_from_path(\"../data/COLIEE2024statute_data-English/train\")\n",
    "out_trainpath = \"../data/COLIEE2024statute_data-English/train.jsonl\"\n",
    "\n",
    "test_path = get_all_files_from_path(\"../data/COLIEE2024statute_data-English/test\")\n",
    "out_testpath = \"../data/COLIEE2024statute_data-English/\"\n",
    "def xml2json(files, out_path):\n",
    "    result = []\n",
    "    for file in files:\n",
    "        data = load_samples(file)\n",
    "        for k in data:\n",
    "            item = {}\n",
    "            if 'index' not in k:\n",
    "                print(k)\n",
    "            item.update({\"id\": k['index']})\n",
    "            item.update({\"content\": zeroShot.replace(\"{{premise}}\", k[\"result\"]).replace(\"{{hypothesis}}\", k[\"content\"])})\n",
    "            if k[\"label\"] == \"Y\": item.update({\"label\": \"true\"})\n",
    "            else: item.update({\"label\": \"false\"})\n",
    "            result.append(item)\n",
    "\n",
    "    if \"R0\" in files[0]:\n",
    "        out_path = out_path+file.split(\"/\")[-1].replace(\".xml\", \"\")+\".jsonl\"\n",
    "    with jsonlines.open(out_path, 'w') as writer:\n",
    "        writer.write_all(result)\n",
    "\n",
    "xml2json(train_path, out_trainpath)\n",
    "xml2json([test_path[0]], out_testpath)\n",
    "xml2json([test_path[1]], out_testpath)\n",
    "xml2json([test_path[2]], out_testpath)\n",
    "xml2json([test_path[3]], out_testpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1c46d7-55a4-4b84-a28c-7ddedb4b693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/congnguyen/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 695 examples [00:00, 47449.97 examples/s]\n",
      "Generating test1 split: 111 examples [00:00, 36676.20 examples/s]\n",
      "Generating test2 split: 81 examples [00:00, 25255.62 examples/s]\n",
      "Generating test3 split: 109 examples [00:00, 33073.80 examples/s]\n",
      "Generating test4 split: 101 examples [00:00, 34949.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'content', 'label'],\n",
      "        num_rows: 695\n",
      "    })\n",
      "    test1: Dataset({\n",
      "        features: ['id', 'content', 'label'],\n",
      "        num_rows: 111\n",
      "    })\n",
      "    test2: Dataset({\n",
      "        features: ['id', 'content', 'label'],\n",
      "        num_rows: 81\n",
      "    })\n",
      "    test3: Dataset({\n",
      "        features: ['id', 'content', 'label'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "    test4: Dataset({\n",
      "        features: ['id', 'content', 'label'],\n",
      "        num_rows: 101\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\":\"../data/COLIEE2024statute_data-English/train.jsonl\", \n",
    "                                          \"test1\": \"../data/COLIEE2024statute_data-English/riteval_R01_en.jsonl\",\n",
    "                                          \"test2\": \"../data/COLIEE2024statute_data-English/riteval_R02_en.jsonl\",\n",
    "                                          \"test3\": \"../data/COLIEE2024statute_data-English/riteval_R03_en.jsonl\",\n",
    "                                          \"test4\": \"../data/COLIEE2024statute_data-English/riteval_R04_en.jsonl\"})\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da750f93-4258-4ab0-ad33-1cf4b3433a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-xxl\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-XL\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"/home/congnguyen/drive/.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c2df6f-7afd-447b-8d2d-2a183844e256",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The maximum total input sequence length after tokenization.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Sequences longer than this will be truncated, sequences shorter will be padded.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenized_inputs \u001b[38;5;241m=\u001b[39m concatenate_datasets([\u001b[43mdataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenizer(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m input_lenghts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tokenized_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# take 85 percentile of max length for better utilization\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"content\"], truncation=True), batched=True, remove_columns=[\"content\", \"label\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 99))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"label\"], truncation=True), batched=True, remove_columns=[\"content\", \"label\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 5))\n",
    "print(f\"Max target length: {max_target_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5719332-36f1-4cad-b06d-0dbbc6f53e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"Classification: \" + item for item in sample[\"content\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"label\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"content\", \"label\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"../output/finetuned/data/train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"../output/finetuned/data/eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf0ee4f-ad20-4c61-ae9f-7d8ff0426e8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:19<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\", cache_dir=\"/home/congnguyen/drive/.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07741043-b112-447c-baec-246adae3188d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/congnguyen/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,874,368 || all params: 11,154,206,720 || trainable%: 0.16921300163961817\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    " r=16,\n",
    " lora_alpha=32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3602aee5-5fd1-4d3d-853e-4e437f3ee526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab665ec-4de5-4309-8e75-d80157d0f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"finetuned-flan-t5-xxl\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31fd60c2-31e0-49ef-922a-ab31d15d93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"../output/finetuned/lora-flan-t5-xxl\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\t# auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7968b451-b4ec-48f2-b83b-da54d16150a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msssnowkool\u001b[0m (\u001b[33mnhmcong\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/congnguyen/drive/Coliee2024/jupyter/wandb/run-20240112_234856-ri7c91dh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nhmcong/finetuned-flan-t5-xxl/runs/ri7c91dh' target=\"_blank\">lilac-dust-4</a></strong> to <a href='https://wandb.ai/nhmcong/finetuned-flan-t5-xxl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nhmcong/finetuned-flan-t5-xxl' target=\"_blank\">https://wandb.ai/nhmcong/finetuned-flan-t5-xxl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nhmcong/finetuned-flan-t5-xxl/runs/ri7c91dh' target=\"_blank\">https://wandb.ai/nhmcong/finetuned-flan-t5-xxl/runs/ri7c91dh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/congnguyen/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/congnguyen/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='144' max='3475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 144/3475 04:26 < 1:44:08, 0.53 it/s, Epoch 0.21/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer.py:2744\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2742\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2743\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:1905\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2763da9-3fb6-4d8b-afb4-13878d39726d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save our LoRA model & tokenizer results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m peft_model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(peft_model_id)\n\u001b[1;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(peft_model_id)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# if you want to save the base model to call\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# trainer.model.base_model.save_pretrained(peft_model_id)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "peft_model_id=\"results\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "# if you want to save the base model to call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5bab4c-e8e1-472e-a15d-21752abff4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2210436/envs/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'content', 'label'],\n",
      "    num_rows: 196\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'content', 'label'],\n",
      "    num_rows: 900\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# train_dataset = load_dataset(\"json\", data_files=\"../data/COLIEE2024statute_data-English/data_full.jsonl\", split='train[0:900]')\n",
    "# test_dataset = load_dataset(\"json\", data_files=\"../data/COLIEE2024statute_data-English/data_full.jsonl\", split='train[900:-1]')\n",
    "# print(test_dataset)\n",
    "# print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62c2399-e958-4310-85c6-0130e63602be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:16<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc.\n",
    "peft_model_id = \"../output/finetuned/lora-flan-t5-xxl_batch?auto_epochs30/checkpoint-2500\"\n",
    "# peft_model_id = \"../output/finetuned/peft_results_batch?auto_epochs30\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, device_map=\"auto\", \n",
    "                                              cache_dir=\"/home/congnguyen/drive/.cache\", load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, cache_dir=\"/home/congnguyen/drive/.cache\")\n",
    "\n",
    "# Load the Lora model\n",
    "# model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map=\"auto\")\n",
    "model.eval()\n",
    "print(\"Peft model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eabe8816-e082-4237-ac9c-2a83c5abfb0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▍                                                                                                                                                         | 1/111 [00:01<02:10,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▊                                                                                                                                                        | 2/111 [00:02<01:59,  1.09s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▏                                                                                                                                                      | 3/111 [00:03<02:25,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▌                                                                                                                                                     | 4/111 [00:05<02:15,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▉                                                                                                                                                    | 5/111 [00:06<02:16,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████████▍                                                                                                                                                  | 6/111 [00:07<02:07,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████████▊                                                                                                                                                 | 7/111 [00:08<02:01,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████████▏                                                                                                                                               | 8/111 [00:09<02:04,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████████████▌                                                                                                                                              | 9/111 [00:11<02:05,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|█████████████▊                                                                                                                                            | 10/111 [00:12<01:59,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████████▎                                                                                                                                          | 11/111 [00:13<01:53,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████████████▋                                                                                                                                         | 12/111 [00:14<01:46,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████████████                                                                                                                                        | 13/111 [00:15<01:48,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████████████▍                                                                                                                                      | 14/111 [00:16<01:45,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████████████▊                                                                                                                                     | 15/111 [00:17<01:35,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████████████████▏                                                                                                                                   | 16/111 [00:17<01:28,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████████████████▌                                                                                                                                  | 17/111 [00:18<01:24,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████████████████▉                                                                                                                                 | 18/111 [00:19<01:16,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████████████████▎                                                                                                                               | 19/111 [00:20<01:25,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████████████████▋                                                                                                                              | 20/111 [00:21<01:29,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████████████████████▏                                                                                                                            | 21/111 [00:22<01:20,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████████▌                                                                                                                           | 22/111 [00:23<01:23,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████████████████████████████▉                                                                                                                          | 23/111 [00:24<01:31,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████████████████████▎                                                                                                                        | 24/111 [00:25<01:35,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████████████████████▋                                                                                                                       | 25/111 [00:26<01:34,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████████████████████                                                                                                                      | 26/111 [00:27<01:30,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████████████████████████████▍                                                                                                                    | 27/111 [00:29<01:31,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████████████████████▊                                                                                                                   | 28/111 [00:30<01:31,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████████████████████████▏                                                                                                                 | 29/111 [00:31<01:31,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████████████████████████▌                                                                                                                | 30/111 [00:32<01:34,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████████████████████████                                                                                                               | 31/111 [00:33<01:31,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████████████████████████████████████▍                                                                                                             | 32/111 [00:34<01:28,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████████████████████▊                                                                                                            | 33/111 [00:35<01:27,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████████████████████████████████████████▏                                                                                                          | 34/111 [00:37<01:27,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████████████████████████████▌                                                                                                         | 35/111 [00:38<01:25,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████████████████████████████▉                                                                                                        | 36/111 [00:39<01:25,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████████████████████████▎                                                                                                      | 37/111 [00:40<01:24,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████████████████████████████▋                                                                                                     | 38/111 [00:41<01:20,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████████████████████████████████                                                                                                    | 39/111 [00:42<01:18,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████████████████████████████████████████████▍                                                                                                  | 40/111 [00:43<01:18,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████████████████████████████████▉                                                                                                 | 41/111 [00:44<01:19,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████████████████████████████████▎                                                                                               | 42/111 [00:46<01:17,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████████████████████████████████▋                                                                                              | 43/111 [00:47<01:17,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████████████                                                                                             | 44/111 [00:48<01:17,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|██████████████████████████████████████████████████████████████▍                                                                                           | 45/111 [00:49<01:14,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████████████████████████████████████████████████▊                                                                                          | 46/111 [00:50<01:11,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████████████████████████████████████▏                                                                                        | 47/111 [00:51<01:07,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████████████████████████████████████████████████████▌                                                                                       | 48/111 [00:52<01:06,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████████████████████████████████████▉                                                                                      | 49/111 [00:53<00:57,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████████████████████████████████████▎                                                                                    | 50/111 [00:54<01:02,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████████████████████████████████████▊                                                                                   | 51/111 [00:55<01:04,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████████████████████████████▏                                                                                 | 52/111 [00:56<01:07,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████████████████████████████████████▌                                                                                | 53/111 [00:58<01:06,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|██████████████████████████████████████████████████████████████████████████▉                                                                               | 54/111 [00:59<01:07,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████████████████████▎                                                                             | 55/111 [01:00<01:05,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████▋                                                                            | 56/111 [01:01<01:01,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████████████████████████████████████████████████████████████████                                                                           | 57/111 [01:02<01:02,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████████████████████████████████████████████████████████████████▍                                                                         | 58/111 [01:04<01:03,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████████████████████████████████████████████████████████████████▊                                                                        | 59/111 [01:05<01:02,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████████████████████████████████████████████▏                                                                      | 60/111 [01:06<00:59,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████████████████████████████████████████████████▋                                                                     | 61/111 [01:07<01:06,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████████████████████████████████                                                                    | 62/111 [01:09<01:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████████████████████████████████████████████████▍                                                                  | 63/111 [01:10<01:01,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 64/111 [01:11<00:59,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████████████████████████████████████████████████████████████████████████▏                                                               | 65/111 [01:12<00:56,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████████████████████████████████████████████████████████████▌                                                              | 66/111 [01:14<00:55,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████▉                                                             | 67/111 [01:15<00:52,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                           | 68/111 [01:16<00:52,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                          | 69/111 [01:17<00:50,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████████████████████████████████████████████████████                                                         | 70/111 [01:18<00:49,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                                                       | 71/111 [01:20<00:48,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████████████████████████████████████████▉                                                      | 72/111 [01:21<00:47,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                    | 73/111 [01:22<00:46,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 74/111 [01:23<00:44,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████                                                  | 75/111 [01:24<00:41,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                | 76/111 [01:25<00:42,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                               | 77/111 [01:27<00:41,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 78/111 [01:28<00:40,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                            | 79/111 [01:29<00:35,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 80/111 [01:30<00:34,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                         | 81/111 [01:31<00:32,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 82/111 [01:32<00:32,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                      | 83/111 [01:33<00:30,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                     | 84/111 [01:34<00:29,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                    | 85/111 [01:36<00:30,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                  | 86/111 [01:37<00:28,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                 | 87/111 [01:38<00:28,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                | 88/111 [01:39<00:27,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 89/111 [01:40<00:25,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 90/111 [01:41<00:24,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                           | 91/111 [01:42<00:18,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                          | 92/111 [01:43<00:17,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                         | 93/111 [01:44<00:17,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 94/111 [01:45<00:15,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                      | 95/111 [01:46<00:15,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 96/111 [01:47<00:14,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                   | 97/111 [01:48<00:13,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 98/111 [01:49<00:12,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 99/111 [01:50<00:11,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊               | 100/111 [01:51<00:10,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 101/111 [01:52<00:10,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌            | 102/111 [01:53<00:09,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 103/111 [01:54<00:08,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎         | 104/111 [01:55<00:07,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 105/111 [01:56<00:06,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 106/111 [01:58<00:05,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 107/111 [01:59<00:04,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 108/111 [02:00<00:03,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 109/111 [02:01<00:02,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 110/111 [02:02<00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 111/111 [02:03<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "result = {}\n",
    "# Load dataset from the hub and get a sample\n",
    "# dataset = load_dataset(\"samsum\")\n",
    "# sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "count = 0\n",
    "\n",
    "# for i in range(len(test_dataset[\"content\"])):\n",
    "for item in tqdm(dataset[\"test1\"]):\n",
    "    id = item[\"id\"]\n",
    "    label = item[\"label\"]\n",
    "    content = item[\"content\"]\n",
    "#     # inputs = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "#     # outputs = model.generate(inputs, max_new_tokens=10)\n",
    "    # =======================\n",
    "    input_ids = tokenizer(content, return_tensors=\"pt\").input_ids.cuda()\n",
    "    # outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.0000001)\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=10)\n",
    "    output_text = format_output(tokenizer.decode(outputs[0]).replace(content, \"\").split(\"\\n\")[-1])\n",
    "    print(output_text)\n",
    "#     # if tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0] == test_dataset[\"label\"][i] and i > 96:\n",
    "#     #     count += 1\n",
    "    answer = \"false\"\n",
    "    if  \"true\" in output_text or \"yes\" in output_text:\n",
    "        answer = \"true\"\n",
    "    if answer == label:\n",
    "        count += 1\n",
    "    if answer == \"true\": answer = \"Y\"\n",
    "    else: answer = \"N\"\n",
    "    result.update({id: answer})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c112655-cf9f-41b8-9920-abea8506078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "0.6936936936936937\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(count/len(dataset[\"test1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a8f228f-6be7-4c36-89b8-4c2690ac599a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R04-02-E': 0,\n",
       " 'R04-04-E': 0,\n",
       " 'R04-04-O': 0,\n",
       " 'R04-05-U': 0,\n",
       " 'R04-09-A': 0,\n",
       " 'R04-09-I': 0,\n",
       " 'R04-09-U': 0,\n",
       " 'R04-10-O': 0,\n",
       " 'R04-14-A': 0,\n",
       " 'R04-16-I': 0,\n",
       " 'R04-16-U': 0,\n",
       " 'R04-16-O': 0,\n",
       " 'R04-18-A': 0,\n",
       " 'R04-18-U': 0,\n",
       " 'R04-18-O': 0,\n",
       " 'R04-19-O': 0,\n",
       " 'R04-20-E': 0,\n",
       " 'R04-22-I': 0,\n",
       " 'R04-23-O': 0,\n",
       " 'R04-25-O': 0,\n",
       " 'R04-26-I': 0,\n",
       " 'R04-26-E': 0,\n",
       " 'R04-27-A': 0,\n",
       " 'R04-27-I': 0,\n",
       " 'R04-29-I': 0,\n",
       " 'R04-29-O': 0,\n",
       " 'R04-36-U': 0,\n",
       " 'R04-37-I': 0,\n",
       " 'total': 0,\n",
       " 'R04-08-A': 0,\n",
       " 'R04-08-O': 0,\n",
       " 'R04-19-E': 0,\n",
       " 'R04-20-A': 0,\n",
       " 'R04-20-O': 0,\n",
       " 'R04-26-U': 0,\n",
       " 'R04-37-U': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "f = open('../data/analysis.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "check = {}\n",
    "for k in data['riteval_R04_en.xml']:\n",
    "    for i in data['riteval_R04_en.xml'][k]:\n",
    "        check.update({i: 0})\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b64511bb-a66e-4f26-a4fb-59f13c628046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "0.801980198019802\n"
     ]
    }
   ],
   "source": [
    "k_result = {}\n",
    "f = open(\"../output/fewshot_detail/prompt_4/riteval_R04_en_acc.txt\", \"r\")\n",
    "label = {}\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    line = line.split(\"\\t\")\n",
    "    k_result.update({line[0]: line[1]})\n",
    "    label.update({line[0]: line[2]})\n",
    "\n",
    "count = 0\n",
    "count2 = 0\n",
    "for k in k_result:\n",
    "    answer = k_result[k]\n",
    "    # if k not in check:\n",
    "    #     # if  result[k] != label[k] and result[k] == k_result[k]:\n",
    "    #     #     count += 1\n",
    "    #     # else:\n",
    "    #     answer = result[k]\n",
    "    #         # print(k, \": \", result[k], \" - \" , k_result[k], \" - \",label[k]) \n",
    "    #         # print(\"==========================\")\n",
    "    # else:\n",
    "    #     answer = k_result[k]\n",
    "    #     # print(k, \": \", result[k], \" + \", k_result[k],  \" - \" ,label[k]) \n",
    "    if answer == label[k]:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "print(count/len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b072df1-6fbf-4b77-9121-34c0182392ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'R01-1-A',\n",
       " 'content': \"According to the given legal reasoning approach.\\nDocument: Article 5\\n(1) A minor must obtain the consent of the minor's legal representative to perform a juridical act;provided, however, that this does not apply to a juridical act for merely acquiring a right or being released from an obligation.\\n(2) A juridical act in contravention of the provisions of the preceding paragraph is voidable.\\n(3) Notwithstanding the provisions of paragraph (1), a minor may freely dispose of property that the legal representative has permitted the minor to dispose of for a specified purpose, to an extent that falls within the scope of that purpose. The same applies if the minor disposes of property that the legal representative has permitted the minor to dispose of without specifying a purpose.\\nQuestion: A contract of sales concluded by a minor may not be rescinded if it relates to daily life, even in cases the consent of the parental authority is not obtained.? True or False\",\n",
       " 'label': 'false'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test1\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a92be72-be0e-4852-a042-9d8f4059df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_peft_model(sample,max_target_length=50):\n",
    "    # generate summary\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)\n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    # decode eval sample\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels\n",
    "\n",
    "# load test dataset from distk\n",
    "test_dataset = load_from_disk(\"../data/eval/\").with_format(\"torch\")\n",
    "\n",
    "# run predictions\n",
    "# this can take ~45 minutes\n",
    "predictions, references = [] , []\n",
    "for sample in tqdm(test_dataset):\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "    predictions.append(p)\n",
    "    references.append(l)\n",
    "\n",
    "# compute metric\n",
    "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "# print results\n",
    "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
    "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
    "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
    "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n",
    "\n",
    "# Rogue1: 50.386161%\n",
    "# rouge2: 24.842412%\n",
    "# rougeL: 41.370130%\n",
    "# rougeLsum: 41.394230%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

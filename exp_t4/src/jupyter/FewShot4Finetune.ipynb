{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff782d06-2da8-46d8-9b43-b844bf439d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def get_all_files_from_path(mypath):\n",
    "    filenames = [join(mypath, f) for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    return filenames\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "def get_article(articles):\n",
    "    result = {}\n",
    "    current_statue = \"(non-statute)\"\n",
    "    for i in re.split(r\"(.*)\", articles.strip()):\n",
    "        if len(i) == 0 or i == \"\\n\":\n",
    "            continue\n",
    "        if re.search(r\"^\\(.*\\)$\", i):\n",
    "            current_statue = i.strip()\n",
    "            if current_statue not in result:\n",
    "                result.update({current_statue: []})\n",
    "        else:\n",
    "            if current_statue not in result:\n",
    "                result.update({current_statue: []})\n",
    "            result[current_statue].append(i)\n",
    "    return result\n",
    "\n",
    "def build_test(filename):\n",
    "    result = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    data = BeautifulSoup(data, \"xml\").find_all('pair')\n",
    "    for i in data:\n",
    "        id = i.get('id')\n",
    "        result.update({id: {}})\n",
    "        result[id].update({\"label\": i.get('label')})\n",
    "        articles = i.find('t1').text.strip()\n",
    "        # articles = get_article(articles)\n",
    "        result[id].update({\"result\": articles})\n",
    "        result[id].update({\"content\": i.find('t2').text.strip()})\n",
    "    return result\n",
    "\n",
    "def write_json(filename, data):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "import xml.etree.ElementTree as Et\n",
    "import glob\n",
    "\n",
    "def format_first_line(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    results = []\n",
    "    for line in lines:\n",
    "        if line[0] == \"\":\n",
    "            continue\n",
    "        if line[0] == \"(\" and line[-1] == \")\":\n",
    "            continue\n",
    "        results.append(line)\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "def load_samples(filexml):\n",
    "    # try:\n",
    "    tree = Et.parse(filexml)\n",
    "    root = tree.getroot()\n",
    "    samples = []\n",
    "    for i in range(0, len(root)):\n",
    "        sample = {'result': []}\n",
    "        for j, e in enumerate(root[i]):\n",
    "            if e.tag == \"t1\":\n",
    "                sample['result'] = format_first_line(e.text.strip())\n",
    "            elif e.tag == \"t2\":\n",
    "                question = e.text.strip()\n",
    "                sample['content'] = question if len(question) > 0 else None\n",
    "        sample.update(\n",
    "            {'index': root[i].attrib['id'], 'label': root[i].attrib.get('label', \"N\")})\n",
    "        # filter the noise samples\n",
    "        if sample['content'] is not None:\n",
    "            samples.append(sample)\n",
    "        else:\n",
    "            print(\"[Important warning] samples {} is ignored\".format(sample))\n",
    "    return samples\n",
    "\n",
    "def load_test_data_samples(path_folder_base, test_id):\n",
    "    data = []\n",
    "    test = load_samples(f\"{path_folder_base}/riteval_{test_id}.xml\")\n",
    "    for file_path in glob.glob(f\"{path_folder_base}/riteval_{test_id}.xml\"):\n",
    "        data = data + load_samples(file_path)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_all_data_samples(path_folder_base):\n",
    "    data = []\n",
    "    for file_path in glob.glob(\"{}/*.xml\".format(path_folder_base)):\n",
    "        data = data + load_samples(file_path)\n",
    "    return data\n",
    "\n",
    "def check_false_labels(pred, false_labels):\n",
    "\tfor label in false_labels:\n",
    "\t\tif label in pred:\n",
    "\t\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def format_output(text):\n",
    "\tCLEANR = re.compile('<.*?>') \n",
    "\tcleantext = re.sub(CLEANR, '', text)\n",
    "\treturn cleantext.strip().lower()\n",
    "\n",
    "def readfile(filename):\n",
    "    f = open(filename)\n",
    "    data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eaf4feb-ed46-4544-9396-82d8f4bbd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_path = \"../data/finetune_exp/fewshot_query\"\n",
    "input_data_path = \"../data/COLIEE2024statute_data-English/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b61f55-65fc-4d8a-bb59-c5f6946e1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base')\n",
    "passage_encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "def dpr(testfile=None, path=\"../data/COLIEE2024statute_data-English/train\"):\n",
    "    datas = get_all_files_from_path(path)\n",
    "    corpus = []\n",
    "    content = []\n",
    "    labels = []\n",
    "    for data in datas:\n",
    "        if testfile != None and testfile in data:\n",
    "            continue\n",
    "        data = load_samples(data)\n",
    "        for item in data:\n",
    "            # corpus.append(item[\"result\"].replace(\"\\n\", \" \").strip())\n",
    "            corpus.append(item[\"result\"].strip())\n",
    "            content.append(item[\"content\"].strip().replace(\".\", \"\"))\n",
    "            labels.append(item[\"label\"].strip())\n",
    "    print(len(corpus))\n",
    "    retrival_passage_embeddings = passage_encoder.encode(corpus)\n",
    "    content_passage_embeddings = passage_encoder.encode(content)\n",
    "    return corpus, content, labels, retrival_passage_embeddings, content_passage_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308dd2f0-2219-43ef-8ce6-4854136b38dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompting(premise, hypothesis, template=None):\n",
    "    text = template.replace(\"{{premise}}\", premise).replace(\"{{hypothesis}}\", hypothesis)\n",
    "    return text\n",
    "\n",
    "def writefile(data, filename):\n",
    "    # Serializing json\n",
    "    json_object = json.dumps(data, indent=1)\n",
    "    # Writing to sample.json\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "        \n",
    "def few_shot_prompting(indexes, corpus, content, labels, prompt_template):\n",
    "    result = \"\"\n",
    "    for i in indexes:\n",
    "        if \"true or false\" in prompt_template.lower():\n",
    "            answer = \"True\"\n",
    "            if \"N\" == labels[i]:\n",
    "                answer = \"False\"\n",
    "        else:\n",
    "            answer = \"Yes\"\n",
    "            if \"N\" == labels[i]:\n",
    "                answer = \"No\"\n",
    "        prompt = prompt_template.replace(\"{{premise}}\", corpus[i]).replace('{{hypothesis}}', content[i]).replace('{{answer}}', answer)\n",
    "        result += prompt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d49bd1-3294-4423-aac6-388508f65b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot = \"Document: {{premise}}\\nQuestion: {{hypothesis}}? True or False\\nAnswer: {{answer}}\\n\\n\"\n",
    "template = \"Document: {{premise}}\\nQuestion: {{hypothesis}}? True or False \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b26d9f-a204-4272-a53a-09e9e79be540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659\n",
      "625\n",
      "637\n",
      "635\n",
      "648\n",
      "641\n",
      "616\n",
      "646\n",
      "654\n",
      "621\n",
      "654\n",
      "646\n",
      "658\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "files = get_all_files_from_path(input_data_path)\n",
    "\n",
    "for file in files:\n",
    "    outfile = file.split(\"/\")[-1].replace(\".xml\", \"\")\n",
    "    outdata = []\n",
    "    data = load_samples(file)\n",
    "    corpus, content, labels, retrival_passage_embeddings, content_passage_embeddings = dpr(outfile)\n",
    "    for item in data:\n",
    "        result = {}\n",
    "        label = item[\"label\"]\n",
    "        if label == \"N\":\n",
    "            label = \"false\"\n",
    "        else:\n",
    "            label = \"true\"\n",
    "        hypothesis = item[\"content\"]\n",
    "        premise = item[\"result\"]\n",
    "        #Important: You must use dot-product, not cosine_similarity\n",
    "        query_embedding = query_encoder.encode(hypothesis)\n",
    "        scores = util.dot_score(query_embedding, content_passage_embeddings)\n",
    "        indexes = torch.topk(scores, 3).indices[0]\n",
    "        few_shot = few_shot_prompting(indexes, corpus, content, labels, fewshot)\n",
    "        text = few_shot + prompting(premise, hypothesis, template)\n",
    "        result.update({\"index\": item[\"index\"]})\n",
    "        result.update({\"content\": hypothesis})\n",
    "        result.update({\"result\": premise})\n",
    "        result.update({\"prompt\": text})\n",
    "        result.update({\"label\": label})\n",
    "        outdata.append(result)\n",
    "    writefile(outdata, f\"{output_data_path}/{outfile}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873f17e4-3105-4cc0-9c82-34c6327c28c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/finetune_exp/fewshot_retrival/riteval_H18_en.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/finetune_exp/fewshot_retrival/riteval_H18_en.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mreadfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 112\u001b[0m, in \u001b[0;36mreadfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadfile\u001b[39m(filename):\n\u001b[0;32m--> 112\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/drive/miniconda3/envs/myenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/finetune_exp/fewshot_retrival/riteval_H18_en.jsonl'"
     ]
    }
   ],
   "source": [
    "data = \"../data/finetune_exp/fewshot_retrival/riteval_H18_en.jsonl\"\n",
    "\n",
    "readfile(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b667c83-a31e-4fad-9610-7dd4f43df36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

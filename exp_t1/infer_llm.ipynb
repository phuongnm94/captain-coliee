{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('/home/s2310409/workspace/coliee-2024/')\n",
    "from utils.misc import get_summary, get_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3e22318bca4ee59ad2c4f55d4d924d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>candidates</th>\n",
       "      <th>negative_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>070318.txt</td>\n",
       "      <td>[015076.txt]</td>\n",
       "      <td>[032432.txt, 071237.txt, 019716.txt, 027423.tx...</td>\n",
       "      <td>[032432.txt, 071237.txt, 019716.txt, 027423.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>077960.txt</td>\n",
       "      <td>[009054.txt, 040860.txt]</td>\n",
       "      <td>[071412.txt, 060516.txt, 024547.txt, 087722.tx...</td>\n",
       "      <td>[071412.txt, 060516.txt, 024547.txt, 087722.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>042319.txt</td>\n",
       "      <td>[093691.txt, 075956.txt, 084953.txt, 022987.txt]</td>\n",
       "      <td>[027719.txt, 067612.txt, 059275.txt, 026904.tx...</td>\n",
       "      <td>[027719.txt, 067612.txt, 059275.txt, 026904.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>041766.txt</td>\n",
       "      <td>[039269.txt]</td>\n",
       "      <td>[071818.txt, 056351.txt, 009599.txt, 046346.tx...</td>\n",
       "      <td>[071818.txt, 056351.txt, 009599.txt, 046346.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>077407.txt</td>\n",
       "      <td>[038669.txt]</td>\n",
       "      <td>[038092.txt, 096647.txt, 056351.txt, 060210.tx...</td>\n",
       "      <td>[038092.txt, 096647.txt, 056351.txt, 060210.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>085079.txt</td>\n",
       "      <td>[044669.txt, 003144.txt]</td>\n",
       "      <td>[080328.txt, 056351.txt, 068423.txt, 041404.tx...</td>\n",
       "      <td>[080328.txt, 056351.txt, 068423.txt, 041404.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>031370.txt</td>\n",
       "      <td>[096341.txt, 060602.txt, 047107.txt, 084522.tx...</td>\n",
       "      <td>[027678.txt, 086122.txt, 060516.txt, 031040.tx...</td>\n",
       "      <td>[027678.txt, 086122.txt, 060516.txt, 031040.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>085828.txt</td>\n",
       "      <td>[004301.txt, 074887.txt, 088994.txt]</td>\n",
       "      <td>[008459.txt, 053850.txt, 003821.txt, 087722.tx...</td>\n",
       "      <td>[008459.txt, 053850.txt, 003821.txt, 087722.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>024957.txt</td>\n",
       "      <td>[015009.txt, 080348.txt]</td>\n",
       "      <td>[066045.txt, 077315.txt, 075868.txt, 022332.tx...</td>\n",
       "      <td>[066045.txt, 077315.txt, 075868.txt, 022332.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>060678.txt</td>\n",
       "      <td>[018625.txt]</td>\n",
       "      <td>[059067.txt, 029016.txt, 058608.txt, 019572.tx...</td>\n",
       "      <td>[059067.txt, 029016.txt, 058608.txt, 019572.tx...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source                                             target  \\\n",
       "0    070318.txt                                       [015076.txt]   \n",
       "1    077960.txt                           [009054.txt, 040860.txt]   \n",
       "2    042319.txt   [093691.txt, 075956.txt, 084953.txt, 022987.txt]   \n",
       "3    041766.txt                                       [039269.txt]   \n",
       "4    077407.txt                                       [038669.txt]   \n",
       "..          ...                                                ...   \n",
       "314  085079.txt                           [044669.txt, 003144.txt]   \n",
       "315  031370.txt  [096341.txt, 060602.txt, 047107.txt, 084522.tx...   \n",
       "316  085828.txt               [004301.txt, 074887.txt, 088994.txt]   \n",
       "317  024957.txt                           [015009.txt, 080348.txt]   \n",
       "318  060678.txt                                       [018625.txt]   \n",
       "\n",
       "                                            candidates  \\\n",
       "0    [032432.txt, 071237.txt, 019716.txt, 027423.tx...   \n",
       "1    [071412.txt, 060516.txt, 024547.txt, 087722.tx...   \n",
       "2    [027719.txt, 067612.txt, 059275.txt, 026904.tx...   \n",
       "3    [071818.txt, 056351.txt, 009599.txt, 046346.tx...   \n",
       "4    [038092.txt, 096647.txt, 056351.txt, 060210.tx...   \n",
       "..                                                 ...   \n",
       "314  [080328.txt, 056351.txt, 068423.txt, 041404.tx...   \n",
       "315  [027678.txt, 086122.txt, 060516.txt, 031040.tx...   \n",
       "316  [008459.txt, 053850.txt, 003821.txt, 087722.tx...   \n",
       "317  [066045.txt, 077315.txt, 075868.txt, 022332.tx...   \n",
       "318  [059067.txt, 029016.txt, 058608.txt, 019572.tx...   \n",
       "\n",
       "                                   negative_candidates  \n",
       "0    [032432.txt, 071237.txt, 019716.txt, 027423.tx...  \n",
       "1    [071412.txt, 060516.txt, 024547.txt, 087722.tx...  \n",
       "2    [027719.txt, 067612.txt, 059275.txt, 026904.tx...  \n",
       "3    [071818.txt, 056351.txt, 009599.txt, 046346.tx...  \n",
       "4    [038092.txt, 096647.txt, 056351.txt, 060210.tx...  \n",
       "..                                                 ...  \n",
       "314  [080328.txt, 056351.txt, 068423.txt, 041404.tx...  \n",
       "315  [027678.txt, 086122.txt, 060516.txt, 031040.tx...  \n",
       "316  [008459.txt, 053850.txt, 003821.txt, 087722.tx...  \n",
       "317  [066045.txt, 077315.txt, 075868.txt, 022332.tx...  \n",
       "318  [059067.txt, 029016.txt, 058608.txt, 019572.tx...  \n",
       "\n",
       "[319 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(dir):\n",
    "    with open(dir, 'r') as fp:\n",
    "        train_data = json.load(fp)\n",
    "\n",
    "    data = []\n",
    "    for key in train_data.keys():\n",
    "        data.append([key, train_data[key]])\n",
    "\n",
    "    return pd.DataFrame(data, columns=['source', 'target'])\n",
    "\n",
    "with open('dataset/c2023/bm25_candidates_test.json', 'r') as fp:\n",
    "    candidate_dict = json.load(fp)\n",
    "\n",
    "data_df = load_data(f'dataset/test.json')\n",
    "data_df['candidates'] = data_df['source'].apply(lambda x: [c for c in candidate_dict[x] if c != x])\n",
    "data_df['negative_candidates'] = data_df.apply(lambda x: [c for c in x['candidates'] if c not in x['target']], axis=1)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "# def reranking_prompting(list_articles, query_content):\n",
    "#     prompting = f\"In bellow articles:  \"\n",
    "#     for a_id in list_articles:\n",
    "#         a_content = get_summary(a_id)\n",
    "#         prompting = prompting + f\"\\n##Article {a_id}: {a_content},\"\n",
    "        \n",
    "#     prompting = prompting +f\"\\n##Question: which articles really relevant to the following article? Answer the article name only. \\n##Article: {query_content}\"\n",
    "#     prompting = prompting + \"\\n##Answer:\"\n",
    "#     return prompting\n",
    "\n",
    "# source = data_df['source'][0]\n",
    "# candidates = candidate_dict[source][0:7]\n",
    "# text = reranking_prompting(candidates, get_summary(source))\n",
    "\n",
    "# encodeds = tokenizer(text, return_tensors=\"pt\")\n",
    "# model_inputs = encodeds.to(device)\n",
    "\n",
    "# generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "# decoded = tokenizer.batch_decode(generated_ids)\n",
    "# print(candidates)\n",
    "# print(decoded[0].replace(text, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['032432.txt', '071237.txt', '019716.txt', '027423.txt', '012462.txt']\n",
      "<s>  Article 032432, Article 071237, and Article 019716. \n",
      "\n",
      "Explanation: The articles pertain to issues related to bias, natural justice, and fairness in decision-making processes, which are also present in the base article. In Article 032432, the Court of Appeal discussed the ability to be impartial when deciding on two occasions and the length of the interrogations during a hearing raising an apprehension of bias. In Article 071237, the Federal Court of Appeal considered the applicant's allegations of his right to a fair hearing and the board's interventions that interfered with his ability to present his case. In Article 019716, the applicants requested materials that were relevant to their claims of bias and breach of procedural fairness. These articles address similar issues and concepts as the base article.</s>\n",
      "['032432', '019716', '071237']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\[', '', text)\n",
    "    text = re.sub(r'\\]', '', text)\n",
    "    return text\n",
    "\n",
    "def instruct_rerank_prompting(base_case, candidates):\n",
    "    prompt = f\"\"\"[INST] You are a helpful legal assistant. You are helping a user to find relevant articles to the following base article.\n",
    "    ## Base Article : \\n{clean_text(get_summary(base_case))}\n",
    "    ## Candidates : \"\"\"\n",
    "    for c in candidates:\n",
    "        content = get_summary(c)\n",
    "        content = clean_text(content)\n",
    "        prompt = prompt + f\"\\nAricle {c.split('.')[0]}: {content}\"\n",
    "    # tokenizer.encode(prompt)\n",
    "    prompt = prompt + f\"\\n## Question : Which articles are closely relevant to the base article? Answer the relevant article name only:[\\INST]\"\n",
    "    return prompt\n",
    "    \n",
    "base_case = data_df['source'][0]\n",
    "candidates = candidate_dict[base_case][0:5]\n",
    "\n",
    "prompt = instruct_rerank_prompting(base_case, candidates)\n",
    "\n",
    "encodeds = tokenizer(prompt, return_tensors=\"pt\")\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(candidates)\n",
    "print(decoded[0].replace(prompt, ''))\n",
    "print(list(set(re.findall(r'\\d{6}', decoded[0].replace(prompt, '')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [13:23<00:00, 16.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'032432': 47, '071237': 35, '012462': 31, '019716': 18, '027423': 14})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for _ in tqdm(range(50)):\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    result.extend(list(set(re.findall(r'\\d{6}', decoded[0].replace(prompt, '')))))\n",
    "\n",
    "# count each article\n",
    "from collections import Counter\n",
    "Counter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on data_df\n",
    "prediction_dict = {}\n",
    "for i in tqdm(range(len(data_df))):\n",
    "    base_case = data_df['source'][i]\n",
    "    prediction_dict[base_case] = []\n",
    "    # group of 5 candidates\n",
    "    for j in range(0, len(data_df['candidates'][i]), 5):\n",
    "        candidates = data_df['candidates'][i][j:j+5]\n",
    "        prompt = instruct_rerank_prompting(base_case, candidates)\n",
    "\n",
    "        encodeds = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        model_inputs = encodeds.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "        decoded = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "        predictions = list(set(re.findall(r'\\d{6}', decoded[0].replace(prompt, ''))))\n",
    "        prediction_dict[base_case].extend(predictions)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_prompting(list_articles, query_content):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                         | 27/7469 [25:34<118:32:04, 57.34s/it]"
     ]
    }
   ],
   "source": [
    "def summarize_prompt(doc):\n",
    "    prompt = f\"\"\"[INST] You are a helpful legal assistant. You are helping a user to summarize case law documents.\n",
    "    ## Article : \\n{doc}\"\"\"\n",
    "    # tokenizer.encode(prompt)\n",
    "    prompt = prompt + f\"\\n## TLDR:[\\INST]\"\n",
    "    return prompt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "list_files = os.listdir('dataset/processed')\n",
    "list_files = [f for f in list_files if f.endswith('.txt')]\n",
    "for file in tqdm(list_files):\n",
    "    with open(f'dataset/processed/{file}', 'r') as fp:\n",
    "        doc = fp.read()\n",
    "    doc = tokenizer.decode(tokenizer.encode(doc, max_length=10000, truncation=True))\n",
    "    prompt = summarize_prompt(doc)\n",
    "    with torch.no_grad():\n",
    "        encodeds = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        model_inputs = encodeds.to(device)\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "        decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    summarized_doc = decoded[0].split('[\\INST]')[1].strip()\n",
    "    with open(f'dataset/mixtral_summarized/{file}', 'w') as fp:\n",
    "        fp.write(summarized_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

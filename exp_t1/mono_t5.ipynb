{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import TrainerCallback, AutoTokenizer\n",
    "\n",
    "os.chdir('/home/s2310409/workspace/coliee-2024/')\n",
    "\n",
    "def load_data(dir):\n",
    "    with open(dir, 'r') as fp:\n",
    "        train_data = json.load(fp)\n",
    "\n",
    "    data = []\n",
    "    for key in train_data.keys():\n",
    "        data.append([key, train_data[key]])\n",
    "\n",
    "    return pd.DataFrame(data, columns=['source', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25 Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(sentences, window_size=10):\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences) - window_size, window_size//2):\n",
    "        chunks.append(\"\\n\".join(sentences[i:i+window_size]))\n",
    "    return chunks\n",
    "\n",
    "with open('dataset/all_data.json') as f:\n",
    "    all_data_dict = json.load(f)\n",
    "\n",
    "word_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "# file_list = sorted(list(all_data_dict.keys()))\n",
    "\n",
    "mode = 'test'\n",
    "\n",
    "file_list = [f for f in os.listdir(f'/home/s2310409/workspace/coliee-2024/dataset-2023/task1/{mode}_files') if f.endswith('.txt')]\n",
    "file_list = [f for f in file_list if f in all_data_dict.keys()]\n",
    "file_list = sorted(file_list)\n",
    "\n",
    "processed_file_dict = {}\n",
    "for file in [f for f in os.listdir(\"dataset/processed\") if not f.startswith('.')]:\n",
    "    processed_file = f\"dataset/processed/{file}\"\n",
    "    with open(processed_file, 'r') as fp:\n",
    "        processed_document = fp.read()\n",
    "        processed_file_dict[file] = {\n",
    "            'sentences': processed_document.split('\\n\\n'),\n",
    "            'processed_document': processed_document\n",
    "        }\n",
    "\n",
    "chunk_dict = {}\n",
    "for file in file_list:\n",
    "    chunks = chunking(processed_file_dict[file]['sentences'])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if len(chunk) > 0:\n",
    "            chunk_dict[f\"{file}_{i}\"] = chunk\n",
    "\n",
    "use_chunk = False\n",
    "\n",
    "if use_chunk:\n",
    "    # bm25 for chunks\n",
    "    corpus = []\n",
    "    chunk_list = sorted(list(chunk_dict.keys()))\n",
    "    for chunk in chunk_list:\n",
    "        corpus.append(chunk_dict[chunk])\n",
    "    tokenized_corpus = [word_tokenizer.tokenize(doc) for doc in corpus]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "else:\n",
    "    # bm25 for whole document\n",
    "    corpus = []\n",
    "    prcessed_list = sorted(file_list)\n",
    "    for file in prcessed_list:\n",
    "        corpus.append(processed_file_dict[file]['processed_document'])\n",
    "    tokenized_corpus = [word_tokenizer.tokenize(doc) for doc in corpus]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query with TF-IDF keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "docs = []\n",
    "for file in file_list:\n",
    "    docs.append(processed_file_dict[file]['processed_document'])\n",
    "\n",
    "count_vec = CountVectorizer(stop_words=stopwords)\n",
    "word_count_vector = count_vec.fit_transform(docs)\n",
    "\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "features = count_vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query on chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_keywords = 25\n",
    "\n",
    "# def extract_query(doc):\n",
    "#     tf_idf_vector=tfidf_transformer.transform(count_vec.transform([doc]))\n",
    "#     sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "#     keywords=extract_topn_from_vector(features,sorted_items, n_keywords)\n",
    "#     return \" \".join(list(keywords.keys()))\n",
    "\n",
    "# query_dict = {}\n",
    "\n",
    "# for file in tqdm(file_list):\n",
    "#     query_dict[file] = extract_query(processed_file_dict[file]['processed_document'])\n",
    "\n",
    "# n_candidates = 150\n",
    "# chunk_candidate_dict = {}\n",
    "# candidate_dict = {}\n",
    "\n",
    "# for file in tqdm(file_list):\n",
    "#     query = query_dict[file]\n",
    "#     tokenized_query = word_tokenizer.tokenize(query)\n",
    "#     results = bm25.get_scores(tokenized_query)\n",
    "#     max_ids = np.argsort(results)[-n_candidates:]\n",
    "\n",
    "#     chunk_candidates = [chunk_list[idx] for idx in max_ids]\n",
    "#     chunk_candidate_dict[file] = chunk_candidates\n",
    "\n",
    "#     document_candidates = [chunk.split('_')[0] for chunk in chunk_candidates]\n",
    "#     candidate_dict[file] = list(set(document_candidates))\n",
    "\n",
    "# data_df = load_data(f'dataset/json/{mode}.json')\n",
    "\n",
    "# data_df['chunk_candidates'] = data_df['source'].apply(lambda x: chunk_candidate_dict[x])\n",
    "# data_df['candidates'] = data_df['source'].apply(lambda x: candidate_dict[x])\n",
    "# data_df['query'] = data_df['source'].apply(lambda x: query_dict[x])\n",
    "\n",
    "# # calculate accuracy metrics for BM25 + TF-IDF\n",
    "# correct = 0\n",
    "# n_retrived = 0\n",
    "# n_relevant = 0\n",
    "\n",
    "# coverages = []\n",
    "\n",
    "# for index, row in data_df.iterrows():\n",
    "#     source = row['source']\n",
    "#     target = row['target']\n",
    "#     preds = row['candidates']\n",
    "#     coverages.append(len(preds))\n",
    "#     n_retrived += len(preds)\n",
    "#     n_relevant += len(target)\n",
    "#     for prediction in preds:\n",
    "#         if prediction in target:\n",
    "#             correct += 1\n",
    "\n",
    "# precision = correct / n_retrived\n",
    "# recall = correct / n_relevant\n",
    "\n",
    "# print(f\"Average # candidates: {np.mean(coverages)}\")\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"F1: {2 * precision * recall / (precision + recall)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query on all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1217/1217 [00:03<00:00, 320.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1217/1217 [00:08<00:00, 135.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # candidates: 150.0\n",
      "Precision: 0.014566353187042842\n",
      "Recall: 0.838748495788207\n",
      "F1: 0.028635401902179496\n"
     ]
    }
   ],
   "source": [
    "n_keywords = 25\n",
    "\n",
    "def extract_query(doc):\n",
    "    tf_idf_vector=tfidf_transformer.transform(count_vec.transform([doc]))\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords=extract_topn_from_vector(features,sorted_items, n_keywords)\n",
    "    return \" \".join(list(keywords.keys()))\n",
    "\n",
    "query_dict = {}\n",
    "\n",
    "for file in tqdm(file_list):\n",
    "    query_dict[file] = extract_query(processed_file_dict[file]['processed_document'])\n",
    "\n",
    "n_candidates = 150\n",
    "candidate_dicts = {}\n",
    "\n",
    "for file in tqdm(file_list):\n",
    "    query = query_dict[file]\n",
    "    tokenized_query = word_tokenizer.tokenize(query)\n",
    "    results = bm25.get_scores(tokenized_query)\n",
    "    max_ids = np.argsort(results)[-n_candidates:]\n",
    "    document_candidates = [file_list[idx] for idx in max_ids]\n",
    "    candidate_dicts[file] = list(set(document_candidates))\n",
    "\n",
    "data_df = load_data(f'dataset/json/{mode}.json')\n",
    "\n",
    "data_df['candidates'] = data_df['source'].apply(lambda x: candidate_dicts[x])\n",
    "data_df['query'] = data_df['source'].apply(lambda x: query_dict[x])\n",
    "\n",
    "# calculate accuracy metrics for BM25 + TF-IDF\n",
    "correct = 0\n",
    "n_retrived = 0\n",
    "n_relevant = 0\n",
    "\n",
    "coverages = []\n",
    "\n",
    "for index, row in data_df.iterrows():\n",
    "    source = row['source']\n",
    "    target = row['target']\n",
    "    preds = row['candidates']\n",
    "    coverages.append(len(preds))\n",
    "    n_retrived += len(preds)\n",
    "    n_relevant += len(target)\n",
    "    for prediction in preds:\n",
    "        if prediction in target:\n",
    "            correct += 1\n",
    "\n",
    "precision = correct / n_retrived\n",
    "recall = correct / n_relevant\n",
    "\n",
    "print(f\"Average # candidates: {np.mean(coverages)}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {2 * precision * recall / (precision + recall)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mono T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>candidates</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>070318.txt</td>\n",
       "      <td>[015076.txt]</td>\n",
       "      <td>[037739.txt, 067598.txt, 082009.txt, 040046.tx...</td>\n",
       "      <td>rpo board guideline adamidis applicant france ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>077960.txt</td>\n",
       "      <td>[009054.txt, 040860.txt]</td>\n",
       "      <td>[044662.txt, 027552.txt, 028766.txt, 019572.tx...</td>\n",
       "      <td>removal child children custody order dunn cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>042319.txt</td>\n",
       "      <td>[093691.txt, 075956.txt, 084953.txt, 022987.txt]</td>\n",
       "      <td>[044662.txt, 010137.txt, 091415.txt, 048470.tx...</td>\n",
       "      <td>beyer cross affidavit prothonotary examination...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>041766.txt</td>\n",
       "      <td>[039269.txt]</td>\n",
       "      <td>[017973.txt, 069618.txt, 091415.txt, 014381.tx...</td>\n",
       "      <td>drug clinical nds health data 002 omitted 08 n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>077407.txt</td>\n",
       "      <td>[038669.txt]</td>\n",
       "      <td>[007118.txt, 048470.txt, 067598.txt, 056351.tx...</td>\n",
       "      <td>communication 23 privilege counsel litigation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>085079.txt</td>\n",
       "      <td>[044669.txt, 003144.txt]</td>\n",
       "      <td>[007118.txt, 025490.txt, 056351.txt, 028766.tx...</td>\n",
       "      <td>cso promotions shephard cst adjudicator commis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>031370.txt</td>\n",
       "      <td>[096341.txt, 060602.txt, 047107.txt, 084522.tx...</td>\n",
       "      <td>[098691.txt, 027552.txt, 082009.txt, 019572.tx...</td>\n",
       "      <td>removal peru applicant irreparable 3d spouse p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>085828.txt</td>\n",
       "      <td>[004301.txt, 074887.txt, 088994.txt]</td>\n",
       "      <td>[029810.txt, 017973.txt, 044930.txt, 098691.tx...</td>\n",
       "      <td>officer applicants india singh riots risk prra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>024957.txt</td>\n",
       "      <td>[015009.txt, 080348.txt]</td>\n",
       "      <td>[049299.txt, 019620.txt, 002842.txt, 053682.tx...</td>\n",
       "      <td>seizure annuity civil 224 code debtor chattels...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>060678.txt</td>\n",
       "      <td>[018625.txt]</td>\n",
       "      <td>[050680.txt, 010137.txt, 019620.txt, 095836.tx...</td>\n",
       "      <td>industrial jurisdiction infringement plaintiff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source                                             target  \\\n",
       "0    070318.txt                                       [015076.txt]   \n",
       "1    077960.txt                           [009054.txt, 040860.txt]   \n",
       "2    042319.txt   [093691.txt, 075956.txt, 084953.txt, 022987.txt]   \n",
       "3    041766.txt                                       [039269.txt]   \n",
       "4    077407.txt                                       [038669.txt]   \n",
       "..          ...                                                ...   \n",
       "314  085079.txt                           [044669.txt, 003144.txt]   \n",
       "315  031370.txt  [096341.txt, 060602.txt, 047107.txt, 084522.tx...   \n",
       "316  085828.txt               [004301.txt, 074887.txt, 088994.txt]   \n",
       "317  024957.txt                           [015009.txt, 080348.txt]   \n",
       "318  060678.txt                                       [018625.txt]   \n",
       "\n",
       "                                            candidates  \\\n",
       "0    [037739.txt, 067598.txt, 082009.txt, 040046.tx...   \n",
       "1    [044662.txt, 027552.txt, 028766.txt, 019572.tx...   \n",
       "2    [044662.txt, 010137.txt, 091415.txt, 048470.tx...   \n",
       "3    [017973.txt, 069618.txt, 091415.txt, 014381.tx...   \n",
       "4    [007118.txt, 048470.txt, 067598.txt, 056351.tx...   \n",
       "..                                                 ...   \n",
       "314  [007118.txt, 025490.txt, 056351.txt, 028766.tx...   \n",
       "315  [098691.txt, 027552.txt, 082009.txt, 019572.tx...   \n",
       "316  [029810.txt, 017973.txt, 044930.txt, 098691.tx...   \n",
       "317  [049299.txt, 019620.txt, 002842.txt, 053682.tx...   \n",
       "318  [050680.txt, 010137.txt, 019620.txt, 095836.tx...   \n",
       "\n",
       "                                                 query  \n",
       "0    rpo board guideline adamidis applicant france ...  \n",
       "1    removal child children custody order dunn cour...  \n",
       "2    beyer cross affidavit prothonotary examination...  \n",
       "3    drug clinical nds health data 002 omitted 08 n...  \n",
       "4    communication 23 privilege counsel litigation ...  \n",
       "..                                                 ...  \n",
       "314  cso promotions shephard cst adjudicator commis...  \n",
       "315  removal peru applicant irreparable 3d spouse p...  \n",
       "316  officer applicants india singh riots risk prra...  \n",
       "317  seizure annuity civil 224 code debtor chattels...  \n",
       "318  industrial jurisdiction infringement plaintiff...  \n",
       "\n",
       "[319 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_summary_data(dir):\n",
    "    summary_data = {}\n",
    "    files = os.listdir(dir)\n",
    "    files = [f for f in files if not f.startswith('.')]\n",
    "    for file in files:\n",
    "        f_path = os.path.join(dir, file)\n",
    "        with open(f_path, 'r') as fp:\n",
    "            summary_data[file] = fp.read()\n",
    "    return summary_data\n",
    "\n",
    "summary_data = load_summary_data('dataset/summarized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(document, query):\n",
    "    return f'##Query: {query} ##Document: {document} ##Relevant:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "ckpt_dir = os.path.join('./train_logs/monot5-large-10k_hns/ckpt/checkpoint-834')\n",
    "tokenizer = AutoTokenizer.from_pretrained('castorini/monot5-large-msmarco-10k')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(ckpt_dir).to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Query: rpo board guideline adamidis applicant france hearing bias ldk actions refugee apprehension member protection immigration recuse port kosovo cvv officer negative findings inference members mosley ##Document: Pelletier, J. : This is an application under section 82.1 of the Immigration Act, R.S.C. 1985, c. The CRDD found that the applicant was not a credible witness. The applicant argues that the CRDD's finding of implausibility was unreasonable. Application for judicial review of the decision of the Convention Refugee Determination Division, dated December 16, 1999. The application for judicial review of ten applicants were heard together because of certain common issues, one of which was whether the applicants had become refugee sur place. Each of the applicants made a claim before the Convention Refugee Determination Division (\"CRDD\") on the basis of well-founded fear of persecution of imputed political opinion The CRDD found that there was insufficient objective grounds to fear harassment by snakeheads upon return to China. The CRDD found that the claimant had not met his burden to demonstrate that the intent or principal effect on him of the Chinese law concerning illegal exit would be persecutory in relation to a Convention ground. [see footnote 11] Counsel for the applicants submitted that it was irrelevant whether the applicants could or could not be identified from the videos submitted in evidence and whether China would or would not know about the present claim for refugee status. Mr. Markaki argued that this question should have been addressed, even in the absence of specific documentary evidence, but on its knowledge of country conditions and the general documentary evidence which describes China as an oppressive regime which does not tolerate any political opposition or criticism of any kind. I agree with Mr. Markaki that the CRDD should have determined how the Chinese government might view making a claim for refugee status \"even in the absence of specific documentary evidence\". I find that the CRDD did not commit a reviewable error in its evaluation of the applicants' sur place claim. [1999] F.C.J. No. 525 (T.D.) (QL) ##Relevant:\n"
     ]
    }
   ],
   "source": [
    "e_id = 0\n",
    "source = data_df.iloc[e_id].source\n",
    "candidates = data_df.iloc[e_id].candidates\n",
    "query = data_df.iloc[e_id].query\n",
    "for candidate in candidates:\n",
    "    candidate_summary = summary_data['015076.txt']\n",
    "    text = prompt(document=tokenizer.decode(tokenizer.encode(candidate_summary, add_special_tokens=False, max_length=450, truncation=True)), query=query)\n",
    "    print(text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GreedySearchEncoderDecoderOutput(sequences=tensor([[   0, 1176,    1]], device='cuda:0'), scores=(tensor([[-31.0093, -17.6671, -17.9352,  ..., -46.2695, -46.2710, -46.3849]],\n",
       "       device='cuda:0'), tensor([[-127.4821,  -40.6877,  -88.1471,  ..., -174.7162, -175.4172,\n",
       "         -175.3853]], device='cuda:0')), encoder_attentions=None, encoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, decoder_hidden_states=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt').to('cuda')\n",
    "outputs = model.generate(**inputs, output_scores=True, return_dict_in_generate=True, max_new_tokens=10)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> false</s>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 6136,    1], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-33.8814, -18.8008, -19.5146,  ..., -48.9525, -48.9488, -49.0627]],\n",
       "        device='cuda:0'),\n",
       " tensor([[-128.0669,  -41.3602,  -88.5412,  ..., -175.1797, -175.8803,\n",
       "          -175.8492]], device='cuda:0'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                        | 0/319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 319/319 [54:18<00:00, 10.21s/it]\n"
     ]
    }
   ],
   "source": [
    "prediction_dict = {}\n",
    "for e_id in tqdm(range(len(data_df))):\n",
    "    source = data_df.iloc[e_id].source\n",
    "    candidates = data_df.iloc[e_id].candidates\n",
    "    query = data_df.iloc[e_id].query\n",
    "    prediction_dict[source] = {\n",
    "        'result':[],\n",
    "        'raw':{}\n",
    "    }\n",
    "    for candidate in candidates:\n",
    "        candidate_summary = summary_data[candidate]\n",
    "        text = prompt(document=tokenizer.decode(tokenizer.encode(candidate_summary, add_special_tokens=False, max_length=450, truncation=True)), query=query)\n",
    "        inputs = tokenizer(text, return_tensors='pt').to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, output_scores=True, return_dict_in_generate=True, max_new_tokens=10)\n",
    "            prediction_dict[source]['raw'] = {\n",
    "                'sequences': list(outputs.sequences[0].cpu().detach().numpy()),\n",
    "                'scores': list([outputs.scores[0].cpu().detach().numpy(), outputs.scores[1].cpu().detach().numpy()])\n",
    "            }\n",
    "            decoded_output = tokenizer.decode(outputs.sequences[0])\n",
    "            if 'true' in decoded_output:\n",
    "                prediction_dict[source]['result'].append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.08880463032844525\n",
      "Precision: 0.016011138183083886\n",
      "Recall: 0.6642599277978339\n",
      "F1: 0.0312685869657575\n"
     ]
    }
   ],
   "source": [
    "data_df['prediction'] = data_df['source'].apply(lambda x: prediction_dict[x]['result'])\n",
    "\n",
    "# calculate accuracy metrics for BM25 + TF-IDF\n",
    "correct = 0\n",
    "n_retrived = 0\n",
    "n_relevant = 0\n",
    "\n",
    "coverages = []\n",
    "\n",
    "for index, row in data_df.iterrows():\n",
    "    source = row['source']\n",
    "    target = row['target']\n",
    "    preds = row['prediction']\n",
    "    coverages.append(len(preds))\n",
    "    n_retrived += len(preds)\n",
    "    n_relevant += len(target)\n",
    "    for prediction in preds:\n",
    "        if prediction in target:\n",
    "            correct += 1\n",
    "\n",
    "precision = correct / n_retrived\n",
    "recall = correct / n_relevant\n",
    "\n",
    "print(f\"Coverage: {np.mean(coverages)/len(file_list)}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {2 * precision * recall / (precision + recall)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 14:32:53 [INFO] env: \n",
      "Using override env var JVM_PATH (/home/s2310409/jdk/lib/server/libjvm.so) to load libjvm.\n",
      "Please report your system information (os version, java\n",
      "version, etc), and the path that works for you, to the\n",
      "PyJNIus project, at https://github.com/kivy/pyjnius/issues.\n",
      "so we can improve the automatic discovery.\n",
      "\n",
      "2024-01-11 14:32:53 [INFO] loader: Loading faiss with AVX2 support.\n",
      "2024-01-11 14:32:53 [INFO] loader: Successfully loaded faiss with AVX2 support.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1217/1217 [00:03<00:00, 320.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1217/1217 [00:08<00:00, 141.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/s2310409/workspace/coliee-2024/modules/pygaggle')\n",
    "from utils.dataset import build_dataset\n",
    "\n",
    "test_dataset = build_dataset(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1217/1217 [00:01<00:00, 720.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2024-01-11 16:08:07,576 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Setting log level to INFO\n",
      "2024-01-11 16:08:07,577 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Starting indexer...\n",
      "2024-01-11 16:08:07,577 INFO  [main] index.IndexCollection (IndexCollection.java:395) - ============ Loading Parameters ============\n",
      "2024-01-11 16:08:07,577 INFO  [main] index.IndexCollection (IndexCollection.java:396) - DocumentCollection path: tmp\n",
      "2024-01-11 16:08:07,577 INFO  [main] index.IndexCollection (IndexCollection.java:397) - CollectionClass: JsonCollection\n",
      "2024-01-11 16:08:07,577 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Generator: DefaultLuceneDocumentGenerator\n",
      "2024-01-11 16:08:07,577 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Threads: 1\n",
      "2024-01-11 16:08:07,577 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Language: en\n",
      "2024-01-11 16:08:07,578 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Stemmer: porter\n",
      "2024-01-11 16:08:07,578 INFO  [main] index.IndexCollection (IndexCollection.java:402) - Keep stopwords? false\n",
      "2024-01-11 16:08:07,578 INFO  [main] index.IndexCollection (IndexCollection.java:403) - Stopwords: null\n",
      "2024-01-11 16:08:07,578 INFO  [main] index.IndexCollection (IndexCollection.java:404) - Store positions? true\n",
      "2024-01-11 16:08:07,578 INFO  [main] index.IndexCollection (IndexCollection.java:405) - Store docvectors? true\n",
      "2024-01-11 16:08:07,578 INFO  [main] index.IndexCollection (IndexCollection.java:406) - Store document \"contents\" field? false\n",
      "2024-01-11 16:08:07,578 INFO  [main] index.IndexCollection (IndexCollection.java:407) - Store document \"raw\" field? true\n",
      "2024-01-11 16:08:07,578 INFO  [main] index.IndexCollection (IndexCollection.java:408) - Additional fields to index: []\n",
      "2024-01-11 16:08:07,578 INFO  [main] index.IndexCollection (IndexCollection.java:409) - Optimize (merge segments)? false\n",
      "2024-01-11 16:08:07,579 INFO  [main] index.IndexCollection (IndexCollection.java:410) - Whitelist: null\n",
      "2024-01-11 16:08:07,579 INFO  [main] index.IndexCollection (IndexCollection.java:411) - Pretokenized?: false\n",
      "2024-01-11 16:08:07,579 INFO  [main] index.IndexCollection (IndexCollection.java:412) - Index path: bm25/test\n",
      "2024-01-11 16:08:07,580 INFO  [main] index.IndexCollection (IndexCollection.java:450) - ============ Indexing Collection ============\n",
      "2024-01-11 16:08:07,831 INFO  [main] index.IndexCollection (IndexCollection.java:565) - Thread pool with 1 threads initialized.\n",
      "2024-01-11 16:08:07,831 INFO  [main] index.IndexCollection (IndexCollection.java:567) - Initializing collection in tmp\n",
      "2024-01-11 16:08:07,833 INFO  [main] index.IndexCollection (IndexCollection.java:576) - 1 file found\n",
      "2024-01-11 16:08:07,833 INFO  [main] index.IndexCollection (IndexCollection.java:577) - Starting to index...\n",
      "2024-01-11 16:08:10,897 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:356) - tmp/candidate.jsonl: 1217 docs added.\n",
      "2024-01-11 16:08:11,342 INFO  [main] index.IndexCollection (IndexCollection.java:633) - Indexing Complete! 1,217 documents indexed\n",
      "2024-01-11 16:08:11,342 INFO  [main] index.IndexCollection (IndexCollection.java:634) - ============ Final Counter Values ============\n",
      "2024-01-11 16:08:11,342 INFO  [main] index.IndexCollection (IndexCollection.java:635) - indexed:            1,217\n",
      "2024-01-11 16:08:11,342 INFO  [main] index.IndexCollection (IndexCollection.java:636) - unindexable:            0\n",
      "2024-01-11 16:08:11,342 INFO  [main] index.IndexCollection (IndexCollection.java:637) - empty:                  0\n",
      "2024-01-11 16:08:11,343 INFO  [main] index.IndexCollection (IndexCollection.java:638) - skipped:                0\n",
      "2024-01-11 16:08:11,343 INFO  [main] index.IndexCollection (IndexCollection.java:639) - errors:                 0\n",
      "2024-01-11 16:08:11,347 INFO  [main] index.IndexCollection (IndexCollection.java:642) - Total 1,217 documents indexed in 00:00:03\n",
      "pyserini.index is deprecated, please use pyserini.index.lucene.\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import subprocess\n",
    "\n",
    "def build_df(mode='test'):\n",
    "    with open('dataset/all_data.json') as f:\n",
    "        all_data_dict = json.load(f)\n",
    "\n",
    "    word_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "    # file_list = sorted(list(all_data_dict.keys()))\n",
    "\n",
    "    file_list = [f for f in os.listdir(f'/home/s2310409/workspace/coliee-2024/dataset-2023/task1/{mode}_files') if f.endswith('.txt')]\n",
    "    file_list = [f for f in file_list if f in all_data_dict.keys()]\n",
    "    file_list = sorted(file_list)\n",
    "\n",
    "    processed_file_dict = {}\n",
    "    for file in [f for f in os.listdir(\"dataset/processed\") if not f.startswith('.')]:\n",
    "        processed_file = f\"dataset/processed/{file}\"\n",
    "        with open(processed_file, 'r') as fp:\n",
    "            processed_document = fp.read()\n",
    "            processed_file_dict[file] = {\n",
    "                'sentences': processed_document.split('\\n\\n'),\n",
    "                'processed_document': processed_document\n",
    "            }\n",
    "\n",
    "    chunk_dict = {}\n",
    "    for file in file_list:\n",
    "        chunks = chunking(processed_file_dict[file]['sentences'])\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if len(chunk) > 0:\n",
    "                chunk_dict[f\"{file}_{i}\"] = chunk\n",
    "\n",
    "    if use_chunk:\n",
    "        # bm25 for chunks\n",
    "        corpus = []\n",
    "        chunk_list = sorted(list(chunk_dict.keys()))\n",
    "        for chunk in chunk_list:\n",
    "            corpus.append(chunk_dict[chunk])\n",
    "        tokenized_corpus = [word_tokenizer.tokenize(doc) for doc in corpus]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "    else:\n",
    "        # bm25 for whole document\n",
    "        corpus = []\n",
    "        prcessed_list = sorted(file_list)\n",
    "        for file in prcessed_list:\n",
    "            corpus.append(processed_file_dict[file]['processed_document'])\n",
    "        tokenized_corpus = [word_tokenizer.tokenize(doc) for doc in corpus]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    docs = []\n",
    "    for file in file_list:\n",
    "        docs.append(processed_file_dict[file]['processed_document'])\n",
    "\n",
    "    count_vec = CountVectorizer(stop_words=stopwords)\n",
    "    word_count_vector = count_vec.fit_transform(docs)\n",
    "\n",
    "\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "    features = count_vec.get_feature_names_out()\n",
    "\n",
    "    n_keywords = 25\n",
    "\n",
    "    def extract_query(doc):\n",
    "        tf_idf_vector=tfidf_transformer.transform(count_vec.transform([doc]))\n",
    "        sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "        keywords=extract_topn_from_vector(features,sorted_items, n_keywords)\n",
    "        return \" \".join(list(keywords.keys()))\n",
    "\n",
    "    query_dict = {}\n",
    "\n",
    "    for file in tqdm(file_list):\n",
    "        query_words = extract_query(processed_file_dict[file]['processed_document'])\n",
    "        query_dict[file] = query_words\n",
    "\n",
    "    n_candidates = 150\n",
    "    candidate_dicts = {}\n",
    "\n",
    "    for file in tqdm(file_list):\n",
    "        query = query_dict[file]\n",
    "        tokenized_query = word_tokenizer.tokenize(query)\n",
    "        results = bm25.get_scores(tokenized_query)\n",
    "        max_ids = np.argsort(results)[-n_candidates:]\n",
    "        document_candidates = [file_list[idx] for idx in max_ids]\n",
    "        candidate_dicts[file] = list(set(document_candidates))\n",
    "\n",
    "    data_df = load_data(f'dataset/json/{mode}.json')\n",
    "    data_df['candidates'] = data_df['source'].apply(lambda x: candidate_dicts[x])\n",
    "    data_df['query'] = data_df['source'].apply(lambda x: query_dict[x])\n",
    "    return data_df\n",
    "\n",
    "def create_bm25_indexes(segment=\"test\"):\n",
    "    tmp_dir = \"tmp\"\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    \n",
    "    indexes_dir = f'bm25/{segment}'\n",
    "    os.makedirs(indexes_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "    with open('dataset/all_data.json') as f:\n",
    "        all_data_dict = json.load(f)\n",
    "\n",
    "    file_list = [f for f in os.listdir(f'/home/s2310409/workspace/coliee-2024/dataset-2023/task1/{segment}_files') if f.endswith('.txt')]\n",
    "    file_list = [f for f in file_list if f in all_data_dict.keys()]\n",
    "    file_list = sorted(file_list)\n",
    "\n",
    "    processed_file_dict = {}\n",
    "    for file in [f for f in os.listdir(\"dataset/processed\") if not f.startswith('.')]:\n",
    "        processed_file = f\"dataset/processed/{file}\"\n",
    "        with open(processed_file, 'r') as fp:\n",
    "            processed_document = fp.read()\n",
    "            processed_file_dict[file] = {\n",
    "                'sentences': processed_document.split('\\n\\n'),\n",
    "                'processed_document': processed_document\n",
    "            }\n",
    "    \n",
    "    # data_df = build_df(mode=segment)\n",
    "    # for case in tqdm(data_df['source'].values):\n",
    "    #     base_case_num = case.split(\".txt\")[0]\n",
    "    #     candidate_cases = data_df[data_df['source'] == case]['candidates'].values[0]\n",
    "    #     for cand_case in candidate_cases:\n",
    "    #         cand_case_data = processed_file_dict[cand_case]['processed_document']\n",
    "    #         cand_num = cand_case.split(\".txt\")[0]\n",
    "    #         dict_ = { \"id\": f\"{base_case_num}_candidate{cand_num}.txt_task2\", \"contents\": cand_case_data}\n",
    "    #         with jsonlines.open(f\"{tmp_dir}/candidate.jsonl\", mode=\"a\") as writer:\n",
    "    #             writer.write(dict_)\n",
    "\n",
    "    for case in tqdm(file_list):\n",
    "        dict_  = { \"id\": f\"{case}\", \"contents\": processed_file_dict[case]['processed_document']}\n",
    "        with jsonlines.open(f\"{tmp_dir}/candidate.jsonl\", mode=\"a\") as writer:\n",
    "            writer.write(dict_)\n",
    "\n",
    "    subprocess.run([\"/home/s2310409/miniconda3/envs/coliee-24/bin/python\", \"-m\", \"pyserini.index\", \"-collection\", \"JsonCollection\",\n",
    "                    \"-generator\", \"DefaultLuceneDocumentGenerator\", \"-threads\", \"1\", \"-input\",\n",
    "                    f\"{tmp_dir}\", \"-index\", f\"{indexes_dir}\", \"-storePositions\", \"-storeDocvectors\",\n",
    "                    \"-storeRaw\"])\n",
    "    \n",
    "create_bm25_indexes(segment=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import LuceneSearcher\n",
    "from collections import defaultdict\n",
    "\n",
    "def predict_bm25(searcher, query, case):\n",
    "    bm25_score = defaultdict(lambda: 0)\n",
    "    hits = searcher.search(query, k=10000)\n",
    "    for hit in hits:\n",
    "        if hit.docid != case:\n",
    "            bm25_score[hit.docid] = max(hit.score, bm25_score[hit.docid])\n",
    "    return bm25_score\n",
    "\n",
    "def predict_all_bm25(dataset_path, bm25_index_path, eval_segment=\"test\",\n",
    "                     k1=None, b=None, topk=None):\n",
    "    searcher = LuceneSearcher(bm25_index_path)\n",
    "    if k1 and b:\n",
    "        print(f\"k1: {k1}, b: {b}\")\n",
    "        searcher.set_bm25(k1, b)\n",
    "\n",
    "    # dataset_path = \"/home/thanhtc/mnt/datasets/COLIEE2023/Task2/data_org\"\n",
    "    corpus_dir, cases_dir, _ = get_task2_data(dataset_path, eval_segment)\n",
    "    bm25_scores = {}\n",
    "    for case in cases_dir:\n",
    "        base_case_data = preprocess_case_data(corpus_dir / case / \"entailed_fragment.txt\")\n",
    "        score = predict_bm25(searcher, base_case_data, case)\n",
    "        if topk is not None:\n",
    "            sorted_score = sorted(score.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "            score = {x[0]: x[1] for x in sorted_score}\n",
    "        bm25_scores[case] = score\n",
    "    return bm25_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "070318.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('070318.txt', 52.63560104370117),\n",
       " ('069215.txt', 14.367500305175781),\n",
       " ('067598.txt', 14.207900047302246),\n",
       " ('025789.txt', 13.736499786376953),\n",
       " ('001589.txt', 13.735300064086914),\n",
       " ('071294.txt', 12.843000411987305),\n",
       " ('039755.txt', 12.746199607849121),\n",
       " ('006254.txt', 12.728699684143066),\n",
       " ('091641.txt', 12.6225004196167),\n",
       " ('025676.txt', 12.134400367736816),\n",
       " ('030534.txt', 12.133399963378906),\n",
       " ('071237.txt', 12.130900382995605),\n",
       " ('073854.txt', 12.090800285339355),\n",
       " ('037739.txt', 11.871399879455566),\n",
       " ('054863.txt', 10.905200004577637),\n",
       " ('024385.txt', 10.859800338745117),\n",
       " ('046028.txt', 10.798100471496582),\n",
       " ('088428.txt', 10.685400009155273),\n",
       " ('098933.txt', 10.271499633789062),\n",
       " ('032432.txt', 10.114299774169922),\n",
       " ('042703.txt', 10.109800338745117),\n",
       " ('069127.txt', 9.991600036621094),\n",
       " ('091307.txt', 9.930399894714355),\n",
       " ('012462.txt', 9.81029987335205),\n",
       " ('055289.txt', 9.728799819946289),\n",
       " ('031467.txt', 9.47249984741211),\n",
       " ('027423.txt', 9.468600273132324),\n",
       " ('048464.txt', 9.387900352478027),\n",
       " ('091360.txt', 9.15839958190918),\n",
       " ('024495.txt', 9.126899719238281),\n",
       " ('059126.txt', 8.9128999710083),\n",
       " ('099397.txt', 8.78030014038086),\n",
       " ('009200.txt', 8.694499969482422),\n",
       " ('071224.txt', 8.599699974060059),\n",
       " ('012393.txt', 8.590800285339355),\n",
       " ('086715.txt', 8.505599975585938),\n",
       " ('070360.txt', 8.462699890136719),\n",
       " ('002230.txt', 8.423100471496582),\n",
       " ('085079.txt', 8.412099838256836),\n",
       " ('054817.txt', 8.366100311279297),\n",
       " ('040002.txt', 8.34570026397705),\n",
       " ('003900.txt', 8.255800247192383),\n",
       " ('093796.txt', 8.200200080871582),\n",
       " ('014578.txt', 8.197500228881836),\n",
       " ('050919.txt', 8.141300201416016),\n",
       " ('056869.txt', 8.104399681091309),\n",
       " ('084131.txt', 8.0447998046875),\n",
       " ('089037.txt', 8.00790023803711),\n",
       " ('018588.txt', 7.9928998947143555),\n",
       " ('085460.txt', 7.978799819946289),\n",
       " ('016657.txt', 7.955100059509277),\n",
       " ('081189.txt', 7.954899787902832),\n",
       " ('028058.txt', 7.940199851989746),\n",
       " ('025486.txt', 7.835700035095215),\n",
       " ('010714.txt', 7.833099842071533),\n",
       " ('047708.txt', 7.744999885559082),\n",
       " ('091264.txt', 7.717299938201904),\n",
       " ('050372.txt', 7.644199848175049),\n",
       " ('047883.txt', 7.634200096130371),\n",
       " ('065761.txt', 7.586999893188477),\n",
       " ('081845.txt', 7.537399768829346),\n",
       " ('081064.txt', 7.525199890136719),\n",
       " ('045055.txt', 7.502399921417236),\n",
       " ('087202.txt', 7.493000030517578),\n",
       " ('082009.txt', 7.451600074768066),\n",
       " ('083049.txt', 7.424499988555908),\n",
       " ('038101.txt', 7.423900127410889),\n",
       " ('016738.txt', 7.3531999588012695),\n",
       " ('017404.txt', 7.34499979019165),\n",
       " ('040407.txt', 7.313399791717529),\n",
       " ('045107.txt', 7.298600196838379),\n",
       " ('003450.txt', 7.270699977874756),\n",
       " ('030585.txt', 7.1656999588012695),\n",
       " ('077598.txt', 7.126999855041504),\n",
       " ('063897.txt', 7.094900131225586),\n",
       " ('091892.txt', 7.091300010681152),\n",
       " ('067441.txt', 7.035699844360352),\n",
       " ('042418.txt', 7.008200168609619),\n",
       " ('047107.txt', 6.966100215911865),\n",
       " ('041334.txt', 6.930300235748291),\n",
       " ('057846.txt', 6.890999794006348),\n",
       " ('081546.txt', 6.825399875640869),\n",
       " ('000491.txt', 6.805699825286865),\n",
       " ('051407.txt', 6.803500175476074),\n",
       " ('062969.txt', 6.796000003814697),\n",
       " ('077181.txt', 6.7941999435424805),\n",
       " ('089005.txt', 6.786200046539307),\n",
       " ('064335.txt', 6.776199817657471),\n",
       " ('003650.txt', 6.752299785614014),\n",
       " ('097187.txt', 6.731299877166748),\n",
       " ('037050.txt', 6.687300205230713),\n",
       " ('040931.txt', 6.684899806976318),\n",
       " ('038039.txt', 6.642000198364258),\n",
       " ('006955.txt', 6.631800174713135),\n",
       " ('099853.txt', 6.623499870300293),\n",
       " ('054236.txt', 6.58650016784668),\n",
       " ('059949.txt', 6.585700035095215),\n",
       " ('019974.txt', 6.57420015335083),\n",
       " ('086750.txt', 6.571899890899658),\n",
       " ('031314.txt', 6.568600177764893),\n",
       " ('033467.txt', 6.542099952697754),\n",
       " ('072949.txt', 6.541500091552734),\n",
       " ('076247.txt', 6.537600040435791),\n",
       " ('042750.txt', 6.515399932861328),\n",
       " ('095654.txt', 6.452600002288818),\n",
       " ('050219.txt', 6.412600040435791),\n",
       " ('040046.txt', 6.397600173950195),\n",
       " ('024903.txt', 6.354400157928467),\n",
       " ('059408.txt', 6.347899913787842),\n",
       " ('062725.txt', 6.330900192260742),\n",
       " ('097311.txt', 6.329100131988525),\n",
       " ('039656.txt', 6.305300235748291),\n",
       " ('018577.txt', 6.273099899291992),\n",
       " ('025207.txt', 6.265999794006348),\n",
       " ('007276.txt', 6.257900238037109),\n",
       " ('021712.txt', 6.240200042724609),\n",
       " ('039099.txt', 6.210299968719482),\n",
       " ('072031.txt', 6.205699920654297),\n",
       " ('079542.txt', 6.17080020904541),\n",
       " ('032764.txt', 6.1427998542785645),\n",
       " ('054676.txt', 6.12470006942749),\n",
       " ('088994.txt', 6.09089994430542),\n",
       " ('040790.txt', 6.077899932861328),\n",
       " ('027678.txt', 6.053100109100342),\n",
       " ('031275.txt', 6.040299892425537),\n",
       " ('067111.txt', 6.033699989318848),\n",
       " ('087722.txt', 6.0329999923706055),\n",
       " ('095636.txt', 6.02209997177124),\n",
       " ('023181.txt', 6.002699851989746),\n",
       " ('051779.txt', 6.000699996948242),\n",
       " ('028889.txt', 5.997300148010254),\n",
       " ('051373.txt', 5.9893999099731445),\n",
       " ('067994.txt', 5.980800151824951),\n",
       " ('011397.txt', 5.964700222015381),\n",
       " ('097029.txt', 5.954899787902832),\n",
       " ('095903.txt', 5.947400093078613),\n",
       " ('005611.txt', 5.945099830627441),\n",
       " ('087919.txt', 5.932400226593018),\n",
       " ('046193.txt', 5.9207000732421875),\n",
       " ('020300.txt', 5.917500019073486),\n",
       " ('042544.txt', 5.876399993896484),\n",
       " ('080575.txt', 5.864799976348877),\n",
       " ('085650.txt', 5.860199928283691),\n",
       " ('020211.txt', 5.839900016784668),\n",
       " ('091204.txt', 5.833199977874756),\n",
       " ('035415.txt', 5.822500228881836),\n",
       " ('010137.txt', 5.80620002746582),\n",
       " ('065833.txt', 5.804299831390381),\n",
       " ('046723.txt', 5.790900230407715),\n",
       " ('087995.txt', 5.7795000076293945),\n",
       " ('023963.txt', 5.764800071716309),\n",
       " ('036281.txt', 5.757500171661377),\n",
       " ('034692.txt', 5.750699996948242),\n",
       " ('065511.txt', 5.74970006942749),\n",
       " ('085582.txt', 5.745999813079834),\n",
       " ('011568.txt', 5.735899925231934),\n",
       " ('055152.txt', 5.729100227355957),\n",
       " ('096308.txt', 5.727399826049805),\n",
       " ('075056.txt', 5.718699932098389),\n",
       " ('039437.txt', 5.6493000984191895),\n",
       " ('074430.txt', 5.647600173950195),\n",
       " ('020051.txt', 5.647500038146973),\n",
       " ('017657.txt', 5.616099834442139),\n",
       " ('063976.txt', 5.61269998550415),\n",
       " ('000871.txt', 5.605299949645996),\n",
       " ('019485.txt', 5.589099884033203),\n",
       " ('060350.txt', 5.55679988861084),\n",
       " ('045430.txt', 5.541299819946289),\n",
       " ('022332.txt', 5.53879976272583),\n",
       " ('084390.txt', 5.536600112915039),\n",
       " ('017679.txt', 5.520500183105469),\n",
       " ('090617.txt', 5.511600017547607),\n",
       " ('061316.txt', 5.507599830627441),\n",
       " ('032939.txt', 5.504300117492676),\n",
       " ('046859.txt', 5.496799945831299),\n",
       " ('043593.txt', 5.486999988555908),\n",
       " ('000084.txt', 5.480599880218506),\n",
       " ('059264.txt', 5.476900100708008),\n",
       " ('009293.txt', 5.458899974822998),\n",
       " ('056402.txt', 5.452499866485596),\n",
       " ('041415.txt', 5.448599815368652),\n",
       " ('071670.txt', 5.4359002113342285),\n",
       " ('072255.txt', 5.42140007019043),\n",
       " ('027141.txt', 5.41540002822876),\n",
       " ('019505.txt', 5.399199962615967),\n",
       " ('026517.txt', 5.389599800109863),\n",
       " ('068895.txt', 5.38539981842041),\n",
       " ('073661.txt', 5.380099773406982),\n",
       " ('074887.txt', 5.3769001960754395),\n",
       " ('094339.txt', 5.372099876403809),\n",
       " ('069460.txt', 5.370299816131592),\n",
       " ('036037.txt', 5.365600109100342),\n",
       " ('077552.txt', 5.365499973297119),\n",
       " ('010784.txt', 5.359799861907959),\n",
       " ('046738.txt', 5.330699920654297),\n",
       " ('074285.txt', 5.314000129699707),\n",
       " ('078899.txt', 5.308899879455566),\n",
       " ('078781.txt', 5.2957000732421875),\n",
       " ('034115.txt', 5.2866997718811035),\n",
       " ('096491.txt', 5.281199932098389),\n",
       " ('092251.txt', 5.261000156402588),\n",
       " ('030209.txt', 5.248000144958496),\n",
       " ('074260.txt', 5.2428998947143555),\n",
       " ('070541.txt', 5.227799892425537),\n",
       " ('090702.txt', 5.2266998291015625),\n",
       " ('043011.txt', 5.224100112915039),\n",
       " ('085872.txt', 5.223299980163574),\n",
       " ('062338.txt', 5.204800128936768),\n",
       " ('000430.txt', 5.204400062561035),\n",
       " ('095142.txt', 5.2006001472473145),\n",
       " ('027927.txt', 5.176300048828125),\n",
       " ('004301.txt', 5.160799980163574),\n",
       " ('038758.txt', 5.160799026489258),\n",
       " ('044053.txt', 5.160200119018555),\n",
       " ('084744.txt', 5.130199909210205),\n",
       " ('000407.txt', 5.127699851989746),\n",
       " ('080870.txt', 5.125800132751465),\n",
       " ('094702.txt', 5.125799179077148),\n",
       " ('015076.txt', 5.125100135803223),\n",
       " ('079855.txt', 5.118199825286865),\n",
       " ('014383.txt', 5.1016998291015625),\n",
       " ('029810.txt', 5.100500106811523),\n",
       " ('055594.txt', 5.0995001792907715),\n",
       " ('085265.txt', 5.094600200653076),\n",
       " ('090471.txt', 5.081099987030029),\n",
       " ('051458.txt', 5.081099033355713),\n",
       " ('013347.txt', 5.073599815368652),\n",
       " ('014123.txt', 5.068999767303467),\n",
       " ('004037.txt', 5.067599773406982),\n",
       " ('000969.txt', 5.057499885559082),\n",
       " ('054099.txt', 5.057199954986572),\n",
       " ('020886.txt', 5.053199768066406),\n",
       " ('092998.txt', 5.047100067138672),\n",
       " ('030621.txt', 5.043799877166748),\n",
       " ('037569.txt', 5.039700031280518),\n",
       " ('031425.txt', 5.034200191497803),\n",
       " ('047775.txt', 5.031000137329102),\n",
       " ('083581.txt', 5.017199993133545),\n",
       " ('013457.txt', 5.01230001449585),\n",
       " ('075820.txt', 4.980599880218506),\n",
       " ('013338.txt', 4.977200031280518),\n",
       " ('067293.txt', 4.963900089263916),\n",
       " ('011383.txt', 4.962900161743164),\n",
       " ('065226.txt', 4.960000038146973),\n",
       " ('083215.txt', 4.957499980926514),\n",
       " ('080946.txt', 4.956500053405762),\n",
       " ('009663.txt', 4.9558000564575195),\n",
       " ('085828.txt', 4.954800128936768),\n",
       " ('011910.txt', 4.954100131988525),\n",
       " ('043963.txt', 4.950200080871582),\n",
       " ('053672.txt', 4.9475998878479),\n",
       " ('065008.txt', 4.904200077056885),\n",
       " ('051357.txt', 4.896200180053711),\n",
       " ('029271.txt', 4.891300201416016),\n",
       " ('076217.txt', 4.877999782562256),\n",
       " ('077945.txt', 4.860300064086914),\n",
       " ('047826.txt', 4.8582000732421875),\n",
       " ('046857.txt', 4.848199844360352),\n",
       " ('062465.txt', 4.844900131225586),\n",
       " ('011599.txt', 4.8420000076293945),\n",
       " ('037517.txt', 4.808499813079834),\n",
       " ('045231.txt', 4.799300193786621),\n",
       " ('049418.txt', 4.7947001457214355),\n",
       " ('077511.txt', 4.792799949645996),\n",
       " ('011014.txt', 4.791600227355957),\n",
       " ('083895.txt', 4.7891998291015625),\n",
       " ('015776.txt', 4.789198875427246),\n",
       " ('079594.txt', 4.7835001945495605),\n",
       " ('055598.txt', 4.781799793243408),\n",
       " ('076957.txt', 4.7667999267578125),\n",
       " ('097850.txt', 4.765200138092041),\n",
       " ('003606.txt', 4.759399890899658),\n",
       " ('085156.txt', 4.752399921417236),\n",
       " ('021891.txt', 4.748600006103516),\n",
       " ('045981.txt', 4.74429988861084),\n",
       " ('015805.txt', 4.732399940490723),\n",
       " ('061516.txt', 4.722300052642822),\n",
       " ('048543.txt', 4.72189998626709),\n",
       " ('036535.txt', 4.718999862670898),\n",
       " ('020549.txt', 4.718299865722656),\n",
       " ('039018.txt', 4.7154998779296875),\n",
       " ('059989.txt', 4.706299781799316),\n",
       " ('086648.txt', 4.6880998611450195),\n",
       " ('056086.txt', 4.669000148773193),\n",
       " ('021482.txt', 4.6641998291015625),\n",
       " ('067809.txt', 4.661200046539307),\n",
       " ('098526.txt', 4.659900188446045),\n",
       " ('040860.txt', 4.653600215911865),\n",
       " ('009599.txt', 4.653500080108643),\n",
       " ('062894.txt', 4.639599800109863),\n",
       " ('077960.txt', 4.638899803161621),\n",
       " ('063443.txt', 4.627799987792969),\n",
       " ('038711.txt', 4.626299858093262),\n",
       " ('054721.txt', 4.624300003051758),\n",
       " ('063191.txt', 4.623799800872803),\n",
       " ('073266.txt', 4.622300148010254),\n",
       " ('066045.txt', 4.60860013961792),\n",
       " ('087518.txt', 4.605800151824951),\n",
       " ('020645.txt', 4.604800224304199),\n",
       " ('067634.txt', 4.604499816894531),\n",
       " ('090843.txt', 4.604100227355957),\n",
       " ('051800.txt', 4.604000091552734),\n",
       " ('050126.txt', 4.596700191497803),\n",
       " ('090651.txt', 4.587900161743164),\n",
       " ('045442.txt', 4.568600177764893),\n",
       " ('020895.txt', 4.5553998947143555),\n",
       " ('064735.txt', 4.552700042724609),\n",
       " ('044697.txt', 4.546899795532227),\n",
       " ('090366.txt', 4.5457000732421875),\n",
       " ('078238.txt', 4.542799949645996),\n",
       " ('088460.txt', 4.542699813842773),\n",
       " ('074044.txt', 4.538000106811523),\n",
       " ('014381.txt', 4.537399768829346),\n",
       " ('081692.txt', 4.52869987487793),\n",
       " ('046231.txt', 4.525599956512451),\n",
       " ('014867.txt', 4.521999835968018),\n",
       " ('097656.txt', 4.51170015335083),\n",
       " ('044478.txt', 4.503200054168701),\n",
       " ('026136.txt', 4.494200229644775),\n",
       " ('048874.txt', 4.489699840545654),\n",
       " ('041720.txt', 4.48829984664917),\n",
       " ('079470.txt', 4.48769998550415),\n",
       " ('098291.txt', 4.481400012969971),\n",
       " ('079713.txt', 4.478799819946289),\n",
       " ('001761.txt', 4.478600025177002),\n",
       " ('087987.txt', 4.46999979019165),\n",
       " ('056351.txt', 4.46750020980835),\n",
       " ('092303.txt', 4.467299938201904),\n",
       " ('068423.txt', 4.452700138092041),\n",
       " ('067252.txt', 4.446300029754639),\n",
       " ('019966.txt', 4.424600124359131),\n",
       " ('049992.txt', 4.408999919891357),\n",
       " ('066333.txt', 4.40749979019165),\n",
       " ('095230.txt', 4.402500152587891),\n",
       " ('024009.txt', 4.396100044250488),\n",
       " ('001638.txt', 4.387199878692627),\n",
       " ('076927.txt', 4.3769001960754395),\n",
       " ('076306.txt', 4.373600006103516),\n",
       " ('071922.txt', 4.369200229644775),\n",
       " ('038281.txt', 4.3592000007629395),\n",
       " ('000576.txt', 4.336599826812744),\n",
       " ('019488.txt', 4.33519983291626),\n",
       " ('052672.txt', 4.304299831390381),\n",
       " ('069325.txt', 4.303100109100342),\n",
       " ('055931.txt', 4.2845001220703125),\n",
       " ('019126.txt', 4.284299850463867),\n",
       " ('055586.txt', 4.279900074005127),\n",
       " ('038624.txt', 4.268899917602539),\n",
       " ('012906.txt', 4.263199806213379),\n",
       " ('043238.txt', 4.259200096130371),\n",
       " ('057664.txt', 4.248899936676025),\n",
       " ('033600.txt', 4.2453999519348145),\n",
       " ('082773.txt', 4.244999885559082),\n",
       " ('088206.txt', 4.240699768066406),\n",
       " ('055295.txt', 4.237199783325195),\n",
       " ('017502.txt', 4.232600212097168),\n",
       " ('082589.txt', 4.231800079345703),\n",
       " ('053350.txt', 4.222899913787842),\n",
       " ('084937.txt', 4.217199802398682),\n",
       " ('032691.txt', 4.206299781799316),\n",
       " ('015272.txt', 4.202600002288818),\n",
       " ('066717.txt', 4.199900150299072),\n",
       " ('050207.txt', 4.196000099182129),\n",
       " ('066564.txt', 4.190299987792969),\n",
       " ('040777.txt', 4.1880998611450195),\n",
       " ('072354.txt', 4.187999248504639),\n",
       " ('053333.txt', 4.186999797821045),\n",
       " ('088815.txt', 4.183499813079834),\n",
       " ('079630.txt', 4.182700157165527),\n",
       " ('060210.txt', 4.1803998947143555),\n",
       " ('010747.txt', 4.1778998374938965),\n",
       " ('084935.txt', 4.17789888381958),\n",
       " ('069812.txt', 4.174799919128418),\n",
       " ('031761.txt', 4.169300079345703),\n",
       " ('093025.txt', 4.166999816894531),\n",
       " ('063800.txt', 4.1631999015808105),\n",
       " ('015388.txt', 4.154900074005127),\n",
       " ('038072.txt', 4.151000022888184),\n",
       " ('079060.txt', 4.147200107574463),\n",
       " ('026404.txt', 4.138599872589111),\n",
       " ('024188.txt', 4.133900165557861),\n",
       " ('009054.txt', 4.125699996948242),\n",
       " ('067835.txt', 4.122700214385986),\n",
       " ('079052.txt', 4.116099834442139),\n",
       " ('049325.txt', 4.111000061035156),\n",
       " ('051741.txt', 4.10830020904541),\n",
       " ('061485.txt', 4.100399971008301),\n",
       " ('059009.txt', 4.084799766540527),\n",
       " ('051262.txt', 4.082699775695801),\n",
       " ('097158.txt', 4.078499794006348),\n",
       " ('077407.txt', 4.078498840332031),\n",
       " ('007844.txt', 4.077000141143799),\n",
       " ('093943.txt', 4.070899963378906),\n",
       " ('071166.txt', 4.069699764251709),\n",
       " ('060581.txt', 4.065700054168701),\n",
       " ('005258.txt', 4.0609002113342285),\n",
       " ('041766.txt', 4.048099994659424),\n",
       " ('049064.txt', 4.0395002365112305),\n",
       " ('062103.txt', 4.039499282836914),\n",
       " ('083028.txt', 4.036799907684326),\n",
       " ('048551.txt', 4.031700134277344),\n",
       " ('094760.txt', 4.029699802398682),\n",
       " ('037404.txt', 4.020999908447266),\n",
       " ('084806.txt', 4.015200138092041),\n",
       " ('066334.txt', 4.011899948120117),\n",
       " ('024756.txt', 4.002900123596191),\n",
       " ('063056.txt', 4.001100063323975),\n",
       " ('057434.txt', 3.999799966812134),\n",
       " ('026535.txt', 3.998300075531006),\n",
       " ('007652.txt', 3.9914000034332275),\n",
       " ('045237.txt', 3.9839000701904297),\n",
       " ('095735.txt', 3.982800006866455),\n",
       " ('028766.txt', 3.9802000522613525),\n",
       " ('019995.txt', 3.9788999557495117),\n",
       " ('022254.txt', 3.9718000888824463),\n",
       " ('007514.txt', 3.969899892807007),\n",
       " ('071330.txt', 3.955199956893921),\n",
       " ('002023.txt', 3.948199987411499),\n",
       " ('005220.txt', 3.9458999633789062),\n",
       " ('053132.txt', 3.9307000637054443),\n",
       " ('008459.txt', 3.9133999347686768),\n",
       " ('071931.txt', 3.9100000858306885),\n",
       " ('091619.txt', 3.888200044631958),\n",
       " ('099777.txt', 3.873500108718872),\n",
       " ('058094.txt', 3.8726000785827637),\n",
       " ('082176.txt', 3.866300106048584),\n",
       " ('041664.txt', 3.863600015640259),\n",
       " ('099731.txt', 3.8550000190734863),\n",
       " ('024496.txt', 3.849100112915039),\n",
       " ('030494.txt', 3.8468000888824463),\n",
       " ('039983.txt', 3.8394999504089355),\n",
       " ('064145.txt', 3.839498996734619),\n",
       " ('002441.txt', 3.838200092315674),\n",
       " ('005806.txt', 3.835700035095215),\n",
       " ('045562.txt', 3.8320999145507812),\n",
       " ('012775.txt', 3.828200101852417),\n",
       " ('090051.txt', 3.821000099182129),\n",
       " ('056867.txt', 3.816499948501587),\n",
       " ('014476.txt', 3.8143999576568604),\n",
       " ('059683.txt', 3.8111000061035156),\n",
       " ('032687.txt', 3.8050999641418457),\n",
       " ('085158.txt', 3.7988998889923096),\n",
       " ('086476.txt', 3.7973999977111816),\n",
       " ('070443.txt', 3.7943999767303467),\n",
       " ('067735.txt', 3.7923998832702637),\n",
       " ('065525.txt', 3.7909998893737793),\n",
       " ('044824.txt', 3.7890000343322754),\n",
       " ('084616.txt', 3.7862000465393066),\n",
       " ('054258.txt', 3.781899929046631),\n",
       " ('041572.txt', 3.774399995803833),\n",
       " ('067093.txt', 3.767400026321411),\n",
       " ('051788.txt', 3.7673990726470947),\n",
       " ('008551.txt', 3.76419997215271),\n",
       " ('048882.txt', 3.7592999935150146),\n",
       " ('063548.txt', 3.7572999000549316),\n",
       " ('001547.txt', 3.7562999725341797),\n",
       " ('039348.txt', 3.7523000240325928),\n",
       " ('002929.txt', 3.746000051498413),\n",
       " ('039269.txt', 3.7437000274658203),\n",
       " ('026347.txt', 3.74180006980896),\n",
       " ('027552.txt', 3.727299928665161),\n",
       " ('054149.txt', 3.725600004196167),\n",
       " ('018445.txt', 3.7195000648498535),\n",
       " ('028444.txt', 3.710700035095215),\n",
       " ('000002.txt', 3.6972999572753906),\n",
       " ('073208.txt', 3.6919000148773193),\n",
       " ('034451.txt', 3.6854000091552734),\n",
       " ('045295.txt', 3.676800012588501),\n",
       " ('021930.txt', 3.6749000549316406),\n",
       " ('000356.txt', 3.673099994659424),\n",
       " ('060516.txt', 3.6717000007629395),\n",
       " ('073214.txt', 3.6672000885009766),\n",
       " ('024547.txt', 3.662600040435791),\n",
       " ('079750.txt', 3.6621999740600586),\n",
       " ('015274.txt', 3.6579999923706055),\n",
       " ('086277.txt', 3.6578991413116455),\n",
       " ('072189.txt', 3.65120005607605),\n",
       " ('035492.txt', 3.6487998962402344),\n",
       " ('098612.txt', 3.648200035095215),\n",
       " ('038549.txt', 3.6421000957489014),\n",
       " ('080485.txt', 3.637200117111206),\n",
       " ('040687.txt', 3.632200002670288),\n",
       " ('089490.txt', 3.631200075149536),\n",
       " ('078730.txt', 3.6224000453948975),\n",
       " ('074686.txt', 3.6196000576019287),\n",
       " ('055537.txt', 3.615999937057495),\n",
       " ('082653.txt', 3.610100030899048),\n",
       " ('049226.txt', 3.609600067138672),\n",
       " ('043506.txt', 3.60509991645813),\n",
       " ('052185.txt', 3.600399971008301),\n",
       " ('089118.txt', 3.598400115966797),\n",
       " ('070586.txt', 3.5936999320983887),\n",
       " ('002859.txt', 3.5927999019622803),\n",
       " ('015010.txt', 3.591200113296509),\n",
       " ('087128.txt', 3.5903000831604004),\n",
       " ('080689.txt', 3.5880000591278076),\n",
       " ('037313.txt', 3.5878000259399414),\n",
       " ('059944.txt', 3.5778000354766846),\n",
       " ('084953.txt', 3.5771000385284424),\n",
       " ('094828.txt', 3.572499990463257),\n",
       " ('000841.txt', 3.565000057220459),\n",
       " ('077828.txt', 3.5636000633239746),\n",
       " ('065753.txt', 3.5566000938415527),\n",
       " ('003946.txt', 3.5555999279022217),\n",
       " ('038902.txt', 3.55049991607666),\n",
       " ('063727.txt', 3.5408999919891357),\n",
       " ('029178.txt', 3.533600091934204),\n",
       " ('011688.txt', 3.52839994430542),\n",
       " ('079879.txt', 3.5139000415802),\n",
       " ('049574.txt', 3.512500047683716),\n",
       " ('046394.txt', 3.509399890899658),\n",
       " ('029970.txt', 3.505000114440918),\n",
       " ('082792.txt', 3.4979000091552734),\n",
       " ('009216.txt', 3.492799997329712),\n",
       " ('030502.txt', 3.4914000034332275),\n",
       " ('043038.txt', 3.482100009918213),\n",
       " ('078222.txt', 3.4702999591827393),\n",
       " ('098736.txt', 3.468899965286255),\n",
       " ('005860.txt', 3.4646999835968018),\n",
       " ('011224.txt', 3.464200019836426),\n",
       " ('060281.txt', 3.462899923324585),\n",
       " ('077142.txt', 3.4605000019073486),\n",
       " ('042893.txt', 3.4600000381469727),\n",
       " ('011683.txt', 3.4595999717712402),\n",
       " ('087785.txt', 3.452399969100952),\n",
       " ('034102.txt', 3.44950008392334),\n",
       " ('082573.txt', 3.44569993019104),\n",
       " ('021157.txt', 3.437999963760376),\n",
       " ('051674.txt', 3.434000015258789),\n",
       " ('012674.txt', 3.4326000213623047),\n",
       " ('032631.txt', 3.4323999881744385),\n",
       " ('086574.txt', 3.429800033569336),\n",
       " ('017608.txt', 3.42930006980896),\n",
       " ('041717.txt', 3.425800085067749),\n",
       " ('050234.txt', 3.422800064086914),\n",
       " ('049280.txt', 3.415299892425537),\n",
       " ('002817.txt', 3.413800001144409),\n",
       " ('060815.txt', 3.41129994392395),\n",
       " ('058999.txt', 3.4107000827789307),\n",
       " ('080420.txt', 3.40939998626709),\n",
       " ('031370.txt', 3.408099889755249),\n",
       " ('028315.txt', 3.4058001041412354),\n",
       " ('091809.txt', 3.4042999744415283),\n",
       " ('080348.txt', 3.4019999504089355),\n",
       " ('068934.txt', 3.400099992752075),\n",
       " ('073477.txt', 3.3966000080108643),\n",
       " ('003507.txt', 3.393699884414673),\n",
       " ('017973.txt', 3.390700101852417),\n",
       " ('002541.txt', 3.377700090408325),\n",
       " ('032947.txt', 3.377500057220459),\n",
       " ('018625.txt', 3.3757998943328857),\n",
       " ('071618.txt', 3.3622000217437744),\n",
       " ('059275.txt', 3.3554999828338623),\n",
       " ('000371.txt', 3.351099967956543),\n",
       " ('007362.txt', 3.350800037384033),\n",
       " ('029479.txt', 3.3422000408172607),\n",
       " ('041813.txt', 3.3405001163482666),\n",
       " ('074293.txt', 3.3373000621795654),\n",
       " ('090193.txt', 3.3348000049591064),\n",
       " ('094359.txt', 3.3346991539001465),\n",
       " ('098691.txt', 3.3341000080108643),\n",
       " ('049954.txt', 3.327899932861328),\n",
       " ('043267.txt', 3.326200008392334),\n",
       " ('093603.txt', 3.3255999088287354),\n",
       " ('085688.txt', 3.3213000297546387),\n",
       " ('049205.txt', 3.3197999000549316),\n",
       " ('057970.txt', 3.315999984741211),\n",
       " ('080776.txt', 3.3129000663757324),\n",
       " ('052887.txt', 3.3118999004364014),\n",
       " ('061987.txt', 3.3071000576019287),\n",
       " ('080845.txt', 3.3060998916625977),\n",
       " ('032668.txt', 3.302000045776367),\n",
       " ('015262.txt', 3.3004000186920166),\n",
       " ('082931.txt', 3.299799919128418),\n",
       " ('046082.txt', 3.2976999282836914),\n",
       " ('077279.txt', 3.296999931335449),\n",
       " ('087767.txt', 3.294600009918213),\n",
       " ('001133.txt', 3.2876999378204346),\n",
       " ('011079.txt', 3.287100076675415),\n",
       " ('093527.txt', 3.2864999771118164),\n",
       " ('016916.txt', 3.286099910736084),\n",
       " ('088628.txt', 3.2788000106811523),\n",
       " ('063299.txt', 3.2653000354766846),\n",
       " ('083743.txt', 3.264699935913086),\n",
       " ('059779.txt', 3.262399911880493),\n",
       " ('069025.txt', 3.2576000690460205),\n",
       " ('040102.txt', 3.2569000720977783),\n",
       " ('098233.txt', 3.2334001064300537),\n",
       " ('042358.txt', 3.2325000762939453),\n",
       " ('030514.txt', 3.2314999103546143),\n",
       " ('019110.txt', 3.2297000885009766),\n",
       " ('032953.txt', 3.22760009765625),\n",
       " ('090145.txt', 3.2265000343322754),\n",
       " ('037999.txt', 3.224900007247925),\n",
       " ('034352.txt', 3.2142999172210693),\n",
       " ('070321.txt', 3.2137999534606934),\n",
       " ('023186.txt', 3.212599992752075),\n",
       " ('038639.txt', 3.2049999237060547),\n",
       " ('011919.txt', 3.204699993133545),\n",
       " ('014465.txt', 3.203399896621704),\n",
       " ('084817.txt', 3.2023000717163086),\n",
       " ('044135.txt', 3.197999954223633),\n",
       " ('087799.txt', 3.1939001083374023),\n",
       " ('003821.txt', 3.193899154663086),\n",
       " ('036512.txt', 3.1923000812530518),\n",
       " ('071589.txt', 3.1833999156951904),\n",
       " ('087891.txt', 3.1814000606536865),\n",
       " ('079772.txt', 3.1760001182556152),\n",
       " ('002768.txt', 3.172600030899048),\n",
       " ('014616.txt', 3.1717000007629395),\n",
       " ('057094.txt', 3.171299934387207),\n",
       " ('035096.txt', 3.1709001064300537),\n",
       " ('054681.txt', 3.168800115585327),\n",
       " ('066726.txt', 3.168299913406372),\n",
       " ('067771.txt', 3.166100025177002),\n",
       " ('076622.txt', 3.159899950027466),\n",
       " ('089208.txt', 3.14520001411438),\n",
       " ('080993.txt', 3.1391000747680664),\n",
       " ('057162.txt', 3.1363000869750977),\n",
       " ('063499.txt', 3.135999917984009),\n",
       " ('073232.txt', 3.134700059890747),\n",
       " ('022014.txt', 3.132200002670288),\n",
       " ('048372.txt', 3.1291000843048096),\n",
       " ('058209.txt', 3.127700090408325),\n",
       " ('048980.txt', 3.1224000453948975),\n",
       " ('091873.txt', 3.107599973678589),\n",
       " ('041555.txt', 3.095099925994873),\n",
       " ('073786.txt', 3.085599899291992),\n",
       " ('031445.txt', 3.0843000411987305),\n",
       " ('092750.txt', 3.0840001106262207),\n",
       " ('077032.txt', 3.0703001022338867),\n",
       " ('078227.txt', 3.0697999000549316),\n",
       " ('037752.txt', 3.058300018310547),\n",
       " ('037691.txt', 3.052299976348877),\n",
       " ('077854.txt', 3.038599967956543),\n",
       " ('048847.txt', 3.037100076675415),\n",
       " ('088344.txt', 3.0313000679016113),\n",
       " ('051101.txt', 3.029599905014038),\n",
       " ('065575.txt', 3.025899887084961),\n",
       " ('021383.txt', 3.023699998855591),\n",
       " ('017722.txt', 3.0216000080108643),\n",
       " ('061288.txt', 3.020400047302246),\n",
       " ('021417.txt', 3.019700050354004),\n",
       " ('016041.txt', 3.015700101852417),\n",
       " ('050117.txt', 3.0023999214172363),\n",
       " ('097826.txt', 2.9992001056671143),\n",
       " ('060054.txt', 2.994800090789795),\n",
       " ('042438.txt', 2.9946999549865723),\n",
       " ('038374.txt', 2.9895999431610107),\n",
       " ('037282.txt', 2.9872000217437744),\n",
       " ('082435.txt', 2.9851999282836914),\n",
       " ('002118.txt', 2.9798998832702637),\n",
       " ('091631.txt', 2.9783999919891357),\n",
       " ('034625.txt', 2.976599931716919),\n",
       " ('069011.txt', 2.970599889755249),\n",
       " ('001275.txt', 2.9647998809814453),\n",
       " ('005869.txt', 2.9600000381469727),\n",
       " ('092503.txt', 2.9588000774383545),\n",
       " ('040585.txt', 2.958799123764038),\n",
       " ('079759.txt', 2.9579999446868896),\n",
       " ('093625.txt', 2.9574999809265137),\n",
       " ('073790.txt', 2.94569993019104),\n",
       " ('003594.txt', 2.9435999393463135),\n",
       " ('065909.txt', 2.942500114440918),\n",
       " ('028297.txt', 2.9326999187469482),\n",
       " ('009880.txt', 2.930999994277954),\n",
       " ('034436.txt', 2.925600051879883),\n",
       " ('035847.txt', 2.924799919128418),\n",
       " ('091490.txt', 2.918600082397461),\n",
       " ('013724.txt', 2.9142000675201416),\n",
       " ('007373.txt', 2.907399892807007),\n",
       " ('064256.txt', 2.906599998474121),\n",
       " ('096431.txt', 2.9059998989105225),\n",
       " ('020677.txt', 2.9035000801086426),\n",
       " ('006399.txt', 2.900399923324585),\n",
       " ('049770.txt', 2.8986001014709473),\n",
       " ('047995.txt', 2.8942999839782715),\n",
       " ('022176.txt', 2.8940999507904053),\n",
       " ('052528.txt', 2.8849000930786133),\n",
       " ('065285.txt', 2.8838999271392822),\n",
       " ('004633.txt', 2.880500078201294),\n",
       " ('024961.txt', 2.879699945449829),\n",
       " ('023182.txt', 2.8785998821258545),\n",
       " ('084522.txt', 2.878200054168701),\n",
       " ('080174.txt', 2.878000020980835),\n",
       " ('061716.txt', 2.876499891281128),\n",
       " ('096788.txt', 2.876300096511841),\n",
       " ('073502.txt', 2.873800039291382),\n",
       " ('071157.txt', 2.872999906539917),\n",
       " ('011890.txt', 2.8726999759674072),\n",
       " ('094665.txt', 2.872699022293091),\n",
       " ('086095.txt', 2.8582000732421875),\n",
       " ('051929.txt', 2.854300022125244),\n",
       " ('000308.txt', 2.842600107192993),\n",
       " ('034525.txt', 2.8422000408172607),\n",
       " ('080028.txt', 2.841200113296509),\n",
       " ('019420.txt', 2.8296000957489014),\n",
       " ('055579.txt', 2.8262999057769775),\n",
       " ('088032.txt', 2.8252999782562256),\n",
       " ('021939.txt', 2.824700117111206),\n",
       " ('028992.txt', 2.8208999633789062),\n",
       " ('044930.txt', 2.814300060272217),\n",
       " ('006674.txt', 2.8142991065979004),\n",
       " ('089216.txt', 2.8129000663757324),\n",
       " ('022098.txt', 2.809799909591675),\n",
       " ('055850.txt', 2.8059000968933105),\n",
       " ('091913.txt', 2.805299997329712),\n",
       " ('045555.txt', 2.7995998859405518),\n",
       " ('048572.txt', 2.7995989322662354),\n",
       " ('077415.txt', 2.7957000732421875),\n",
       " ('079863.txt', 2.791300058364868),\n",
       " ('044380.txt', 2.790800094604492),\n",
       " ('092507.txt', 2.7857000827789307),\n",
       " ('060523.txt', 2.7771999835968018),\n",
       " ('070546.txt', 2.77239990234375),\n",
       " ('056801.txt', 2.77020001411438),\n",
       " ('021669.txt', 2.7679998874664307),\n",
       " ('019887.txt', 2.7676000595092773),\n",
       " ('062612.txt', 2.763400077819824),\n",
       " ('094010.txt', 2.7607998847961426),\n",
       " ('042718.txt', 2.759999990463257),\n",
       " ('063140.txt', 2.759399890899658),\n",
       " ('090701.txt', 2.749000072479248),\n",
       " ('071818.txt', 2.748500108718872),\n",
       " ('029065.txt', 2.7476000785827637),\n",
       " ('017883.txt', 2.7411999702453613),\n",
       " ('081034.txt', 2.732100009918213),\n",
       " ('044580.txt', 2.7297000885009766),\n",
       " ('017327.txt', 2.727099895477295),\n",
       " ('031573.txt', 2.726900100708008),\n",
       " ('023080.txt', 2.7255001068115234),\n",
       " ('075367.txt', 2.7251999378204346),\n",
       " ('080328.txt', 2.7223000526428223),\n",
       " ('080241.txt', 2.7211999893188477),\n",
       " ('029742.txt', 2.715399980545044),\n",
       " ('026350.txt', 2.714400053024292),\n",
       " ('091634.txt', 2.7040998935699463),\n",
       " ('036344.txt', 2.694000005722046),\n",
       " ('077496.txt', 2.69320011138916),\n",
       " ('026768.txt', 2.6842000484466553),\n",
       " ('043615.txt', 2.6828999519348145),\n",
       " ('093901.txt', 2.6816000938415527),\n",
       " ('069607.txt', 2.677500009536743),\n",
       " ('035369.txt', 2.6760001182556152),\n",
       " ('038669.txt', 2.669100046157837),\n",
       " ('088071.txt', 2.6689999103546143),\n",
       " ('042867.txt', 2.664799928665161),\n",
       " ('029230.txt', 2.661099910736084),\n",
       " ('030815.txt', 2.6589999198913574),\n",
       " ('001457.txt', 2.6526999473571777),\n",
       " ('002842.txt', 2.650099992752075),\n",
       " ('030565.txt', 2.6414999961853027),\n",
       " ('030506.txt', 2.6386001110076904),\n",
       " ('091415.txt', 2.634500026702881),\n",
       " ('049283.txt', 2.623500108718872),\n",
       " ('095325.txt', 2.621299982070923),\n",
       " ('003572.txt', 2.6191000938415527),\n",
       " ('006921.txt', 2.6154000759124756),\n",
       " ('078642.txt', 2.6143999099731445),\n",
       " ('047256.txt', 2.6029000282287598),\n",
       " ('053850.txt', 2.600800037384033),\n",
       " ('077659.txt', 2.5961999893188477),\n",
       " ('056725.txt', 2.59060001373291),\n",
       " ('079008.txt', 2.5899999141693115),\n",
       " ('039319.txt', 2.587399959564209),\n",
       " ('090411.txt', 2.5782999992370605),\n",
       " ('008283.txt', 2.57669997215271),\n",
       " ('079923.txt', 2.5678000450134277),\n",
       " ('091997.txt', 2.5676000118255615),\n",
       " ('096293.txt', 2.5631000995635986),\n",
       " ('002856.txt', 2.549999952316284),\n",
       " ('018646.txt', 2.5497000217437744),\n",
       " ('022837.txt', 2.5488998889923096),\n",
       " ('072867.txt', 2.5429999828338623),\n",
       " ('096341.txt', 2.54259991645813),\n",
       " ('000349.txt', 2.5415000915527344),\n",
       " ('095707.txt', 2.539299964904785),\n",
       " ('096717.txt', 2.539199113845825),\n",
       " ('023740.txt', 2.5373001098632812),\n",
       " ('021081.txt', 2.5343000888824463),\n",
       " ('003940.txt', 2.5322000980377197),\n",
       " ('094035.txt', 2.529900074005127),\n",
       " ('007375.txt', 2.5295000076293945),\n",
       " ('022412.txt', 2.5285000801086426),\n",
       " ('028729.txt', 2.5246999263763428),\n",
       " ('088526.txt', 2.5230000019073486),\n",
       " ('024164.txt', 2.518899917602539),\n",
       " ('019712.txt', 2.518699884414673),\n",
       " ('098992.txt', 2.517899990081787),\n",
       " ('015851.txt', 2.515700101852417),\n",
       " ('016255.txt', 2.513000011444092),\n",
       " ('070592.txt', 2.5104000568389893),\n",
       " ('071594.txt', 2.5058999061584473),\n",
       " ('097225.txt', 2.5009000301361084),\n",
       " ('081477.txt', 2.4992001056671143),\n",
       " ('062047.txt', 2.498800039291382),\n",
       " ('046346.txt', 2.4935998916625977),\n",
       " ('074687.txt', 2.4865000247955322),\n",
       " ('087944.txt', 2.4765000343322754),\n",
       " ('049359.txt', 2.4742000102996826),\n",
       " ('074097.txt', 2.4607999324798584),\n",
       " ('083611.txt', 2.460400104522705),\n",
       " ('062202.txt', 2.4598000049591064),\n",
       " ('034391.txt', 2.4563000202178955),\n",
       " ('027941.txt', 2.4465999603271484),\n",
       " ('026705.txt', 2.44569993019104),\n",
       " ('003146.txt', 2.433500051498413),\n",
       " ('016509.txt', 2.4307000637054443),\n",
       " ('068642.txt', 2.4277000427246094),\n",
       " ('010524.txt', 2.427500009536743),\n",
       " ('022499.txt', 2.4249000549316406),\n",
       " ('023598.txt', 2.4193999767303467),\n",
       " ('051807.txt', 2.41759991645813),\n",
       " ('092757.txt', 2.417099952697754),\n",
       " ('012505.txt', 2.4128000736236572),\n",
       " ('028791.txt', 2.4117000102996826),\n",
       " ('045064.txt', 2.407099962234497),\n",
       " ('029905.txt', 2.405400037765503),\n",
       " ('035451.txt', 2.3917999267578125),\n",
       " ('058364.txt', 2.371299982070923),\n",
       " ('063702.txt', 2.365000009536743),\n",
       " ('015602.txt', 2.3615000247955322),\n",
       " ('075225.txt', 2.3605000972747803),\n",
       " ('099485.txt', 2.360499143600464),\n",
       " ('042431.txt', 2.355299949645996),\n",
       " ('041419.txt', 2.3515000343322754),\n",
       " ('013913.txt', 2.3485000133514404),\n",
       " ('012134.txt', 2.3350000381469727),\n",
       " ('082069.txt', 2.3303000926971436),\n",
       " ('023988.txt', 2.3232998847961426),\n",
       " ('045465.txt', 2.322200059890747),\n",
       " ('086561.txt', 2.3183999061584473),\n",
       " ('087172.txt', 2.3134000301361084),\n",
       " ('055790.txt', 2.309499979019165),\n",
       " ('051371.txt', 2.3025999069213867),\n",
       " ('093675.txt', 2.291800022125244),\n",
       " ('046646.txt', 2.286799907684326),\n",
       " ('055351.txt', 2.286600112915039),\n",
       " ('028647.txt', 2.283799886703491),\n",
       " ('026053.txt', 2.261199951171875),\n",
       " ('066843.txt', 2.2578999996185303),\n",
       " ('009941.txt', 2.255199909210205),\n",
       " ('059307.txt', 2.2546000480651855),\n",
       " ('036398.txt', 2.2500998973846436),\n",
       " ('014531.txt', 2.2407000064849854),\n",
       " ('070627.txt', 2.239300012588501),\n",
       " ('019981.txt', 2.2360999584198),\n",
       " ('096647.txt', 2.23599910736084),\n",
       " ('059103.txt', 2.2314999103546143),\n",
       " ('000457.txt', 2.2298998832702637),\n",
       " ('053682.txt', 2.2258999347686768),\n",
       " ('081169.txt', 2.216399908065796),\n",
       " ('081100.txt', 2.215399980545044),\n",
       " ('017268.txt', 2.2019999027252197),\n",
       " ('057947.txt', 2.1930999755859375),\n",
       " ('048217.txt', 2.171799898147583),\n",
       " ('041735.txt', 2.1686999797821045),\n",
       " ('069572.txt', 2.1658999919891357),\n",
       " ('049817.txt', 2.1656999588012695),\n",
       " ('018554.txt', 2.16510009765625),\n",
       " ('000298.txt', 2.1633999347686768),\n",
       " ('042829.txt', 2.1603000164031982),\n",
       " ('058831.txt', 2.1591999530792236),\n",
       " ('012732.txt', 2.1579999923706055),\n",
       " ('058862.txt', 2.156100034713745),\n",
       " ('076852.txt', 2.152400016784668),\n",
       " ('097454.txt', 2.1475000381469727),\n",
       " ('019572.txt', 2.1456000804901123),\n",
       " ('068767.txt', 2.1435000896453857),\n",
       " ('044662.txt', 2.1419999599456787),\n",
       " ('050755.txt', 2.1373000144958496),\n",
       " ('020200.txt', 2.13070011138916),\n",
       " ('063802.txt', 2.1298000812530518),\n",
       " ('095649.txt', 2.1157000064849854),\n",
       " ('001380.txt', 2.1078999042510986),\n",
       " ('095164.txt', 2.1078989505767822),\n",
       " ('086122.txt', 2.0999999046325684),\n",
       " ('067678.txt', 2.088399887084961),\n",
       " ('053992.txt', 2.0843000411987305),\n",
       " ('008618.txt', 2.0766000747680664),\n",
       " ('090829.txt', 2.0696001052856445),\n",
       " ('048447.txt', 2.0676000118255615),\n",
       " ('015723.txt', 2.0660998821258545),\n",
       " ('005147.txt', 2.060800075531006),\n",
       " ('002484.txt', 2.0560998916625977),\n",
       " ('029016.txt', 2.0434999465942383),\n",
       " ('088184.txt', 2.0429999828338623),\n",
       " ('072381.txt', 2.039900064468384),\n",
       " ('013354.txt', 2.038800001144409),\n",
       " ('020781.txt', 2.0369999408721924),\n",
       " ('066696.txt', 2.0350000858306885),\n",
       " ('033849.txt', 2.025700092315674),\n",
       " ('094886.txt', 2.0237998962402344),\n",
       " ('002468.txt', 2.0155999660491943),\n",
       " ('011618.txt', 2.002500057220459),\n",
       " ('099319.txt', 2.001300096511841),\n",
       " ('027728.txt', 1.9926999807357788),\n",
       " ('030798.txt', 1.9884999990463257),\n",
       " ('069056.txt', 1.9884990453720093),\n",
       " ('070622.txt', 1.979699969291687),\n",
       " ('087571.txt', 1.979200005531311),\n",
       " ('033033.txt', 1.979099988937378),\n",
       " ('036418.txt', 1.9789999723434448),\n",
       " ('086362.txt', 1.969599962234497),\n",
       " ('027719.txt', 1.968000054359436),\n",
       " ('007118.txt', 1.9591000080108643),\n",
       " ('075323.txt', 1.9559999704360962),\n",
       " ('051563.txt', 1.950700044631958),\n",
       " ('092889.txt', 1.9429999589920044),\n",
       " ('082586.txt', 1.9428000450134277),\n",
       " ('074502.txt', 1.9417999982833862),\n",
       " ('049964.txt', 1.940600037574768),\n",
       " ('092542.txt', 1.940000057220459),\n",
       " ('017489.txt', 1.9240000247955322),\n",
       " ('023355.txt', 1.9212000370025635),\n",
       " ('036068.txt', 1.913699984550476),\n",
       " ('052545.txt', 1.9038000106811523),\n",
       " ('062619.txt', 1.895900011062622),\n",
       " ('051660.txt', 1.8941999673843384),\n",
       " ('044501.txt', 1.8935999870300293),\n",
       " ('024976.txt', 1.8925000429153442),\n",
       " ('078006.txt', 1.8909000158309937),\n",
       " ('095836.txt', 1.8835999965667725),\n",
       " ('025490.txt', 1.881600022315979),\n",
       " ('037515.txt', 1.8811999559402466),\n",
       " ('091777.txt', 1.8767999410629272),\n",
       " ('078190.txt', 1.8739999532699585),\n",
       " ('044154.txt', 1.8645000457763672),\n",
       " ('046732.txt', 1.8631000518798828),\n",
       " ('031928.txt', 1.858299970626831),\n",
       " ('082594.txt', 1.8564000129699707),\n",
       " ('041713.txt', 1.854099988937378),\n",
       " ('083249.txt', 1.8495999574661255),\n",
       " ('045271.txt', 1.8436000347137451),\n",
       " ('085948.txt', 1.8381999731063843),\n",
       " ('075868.txt', 1.8350000381469727),\n",
       " ('007416.txt', 1.8341000080108643),\n",
       " ('031040.txt', 1.8331999778747559),\n",
       " ('021206.txt', 1.8313000202178955),\n",
       " ('010656.txt', 1.8306000232696533),\n",
       " ('093985.txt', 1.8214000463485718),\n",
       " ('068200.txt', 1.819599986076355),\n",
       " ('026238.txt', 1.795699954032898),\n",
       " ('088922.txt', 1.7825000286102295),\n",
       " ('001653.txt', 1.7711000442504883),\n",
       " ('058746.txt', 1.7675000429153442),\n",
       " ('011785.txt', 1.766800045967102),\n",
       " ('055744.txt', 1.745300054550171),\n",
       " ('049113.txt', 1.742900013923645),\n",
       " ('023549.txt', 1.7408000230789185),\n",
       " ('067612.txt', 1.7319999933242798),\n",
       " ('067321.txt', 1.7319990396499634),\n",
       " ('033376.txt', 1.7271000146865845),\n",
       " ('086054.txt', 1.7157000303268433),\n",
       " ('077339.txt', 1.714900016784668),\n",
       " ('059048.txt', 1.7039999961853027),\n",
       " ('007053.txt', 1.7030999660491943),\n",
       " ('043109.txt', 1.695199966430664),\n",
       " ('091396.txt', 1.691499948501587),\n",
       " ('052883.txt', 1.6829999685287476),\n",
       " ('005388.txt', 1.6764999628067017),\n",
       " ('032648.txt', 1.6753000020980835),\n",
       " ('006318.txt', 1.6725000143051147),\n",
       " ('059994.txt', 1.6648000478744507),\n",
       " ('039650.txt', 1.6612999439239502),\n",
       " ('033411.txt', 1.6598999500274658),\n",
       " ('019075.txt', 1.6589000225067139),\n",
       " ('002614.txt', 1.6571999788284302),\n",
       " ('044708.txt', 1.6520999670028687),\n",
       " ('025976.txt', 1.6490999460220337),\n",
       " ('026266.txt', 1.6464999914169312),\n",
       " ('015364.txt', 1.6446000337600708),\n",
       " ('065808.txt', 1.6375999450683594),\n",
       " ('029288.txt', 1.6369999647140503),\n",
       " ('062333.txt', 1.6354000568389893),\n",
       " ('062704.txt', 1.6274000406265259),\n",
       " ('082264.txt', 1.6224000453948975),\n",
       " ('052084.txt', 1.6216000318527222),\n",
       " ('016203.txt', 1.6208000183105469),\n",
       " ('036271.txt', 1.617300033569336),\n",
       " ('029923.txt', 1.6145999431610107),\n",
       " ('098335.txt', 1.608199954032898),\n",
       " ('001704.txt', 1.604200005531311),\n",
       " ('056239.txt', 1.603700041770935),\n",
       " ('024050.txt', 1.6018999814987183),\n",
       " ('041404.txt', 1.5856000185012817),\n",
       " ('005266.txt', 1.5855990648269653),\n",
       " ('023997.txt', 1.5786999464035034),\n",
       " ('079943.txt', 1.569000005722046),\n",
       " ('021362.txt', 1.5662000179290771),\n",
       " ('014798.txt', 1.5642000436782837),\n",
       " ('091283.txt', 1.5612000226974487),\n",
       " ('042625.txt', 1.5568000078201294),\n",
       " ('067386.txt', 1.5536999702453613),\n",
       " ('027381.txt', 1.5470999479293823),\n",
       " ('071894.txt', 1.547098994255066),\n",
       " ('063256.txt', 1.542199969291687),\n",
       " ('043463.txt', 1.5379999876022339),\n",
       " ('092407.txt', 1.5356999635696411),\n",
       " ('051306.txt', 1.5240999460220337),\n",
       " ...]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher = LuceneSearcher(\"bm25/test\")\n",
    "\n",
    "base_id = data_df.iloc[0].source\n",
    "base_query = data_df.iloc[0].query\n",
    "cand_case = data_df.iloc[0].candidates[0]\n",
    "\n",
    "hits = searcher.search(base_query, k=10000)\n",
    "print(base_id)\n",
    "[(h.docid, h.score) for h in hits]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
